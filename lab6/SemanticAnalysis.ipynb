{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b84610c7",
   "metadata": {},
   "source": [
    "# Semantic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e29103d",
   "metadata": {},
   "source": [
    "TF-IDF vectors can be used to anlyze both words and n-grams.\n",
    "\n",
    "We are currently aiming at finding a meaning of sentences by looking on the general word content. We will construct *topic*/*semantic* vectors that carry meaning of a sentence and not its statistics.\n",
    "\n",
    "\n",
    "Note that rephrased sentence can have the same meaning having different TF-IDF vectors! Therefore closednes measures as cos-similarity cannot compare semantics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82254874",
   "metadata": {},
   "source": [
    "LSA = Latent Semantic Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd56600a",
   "metadata": {},
   "source": [
    "Some important notions in analysing semantics (LSA):\n",
    "\n",
    "- Polysemy - The existence of words and phrases with more than one meaning\n",
    "- Homonyms - Words with the same spelling and pronunciation, but different meanings\n",
    "- Zeugma - Use of two meanings of a word simultaneously in the same sentence\n",
    "- Homographs - Words spelled the same, but with different pronunciations and meanings\n",
    "- Homophones -Words with the same pronunciation, but different spellings and meanings (an NLP challenge with voice interfaces)\n",
    "\n",
    "All these problems are issues in LSA."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44a79d7c",
   "metadata": {},
   "source": [
    "## Initial experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7dc61b",
   "metadata": {},
   "source": [
    "We construct topics/semnatic vectors by weighting contibutions from different words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "52aae6a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tfidf =  {'cat': 0.08219887120266478, 'dog': 0.6434922923613229, 'apple': 0.7259162900863801, 'lion': 0.2601513375554295, 'NYC': 0.5989888276780737, 'love': 0.2718156894499929}\n",
      "petness =  0.15227272142358014\n",
      "animalness =  0.1627704699482836\n",
      "cityness =  0.3814948778096369\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "topic = {}\n",
    "tfidf = dict(list(zip('cat dog apple lion NYC love'.split(), np.random.rand(6))))\n",
    "\n",
    "print(\"tfidf = \", tfidf)\n",
    "\n",
    "topic['petness'] = (.3* tfidf['cat'] +\\\n",
    "    .3 * tfidf['dog'] +\\\n",
    "     0 * tfidf['apple'] +\\\n",
    "     0 * tfidf['lion'] -\\\n",
    "     .2 * tfidf['NYC'] +\\\n",
    "     .2 * tfidf['love'])\n",
    "\n",
    "topic['animalness'] = (.1 * tfidf['cat'] +\\\n",
    "    .1 * tfidf['dog'] -\\\n",
    "    .1 * tfidf['apple'] +\\\n",
    "    .5 * tfidf['lion'] +\\\n",
    "    .1 * tfidf['NYC'] -\\\n",
    "    .1 * tfidf['love'])\n",
    "\n",
    "topic['cityness'] = ( 0 * tfidf['cat'] -\\\n",
    "    .1 * tfidf['dog'] +\\\n",
    "    .2 * tfidf['apple'] -\\\n",
    "    .1 * tfidf['lion'] +\\\n",
    "    .5 * tfidf['NYC'] +\\\n",
    "    .1 * tfidf['love'])\n",
    "\n",
    "print(\"petness = \", topic['petness'])\n",
    "print(\"animalness = \", topic['animalness'])\n",
    "print(\"cityness = \", topic['cityness'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f36df6c",
   "metadata": {},
   "source": [
    "**Note** Negative weights represnet opposite influence of a word on a topic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fb414c",
   "metadata": {},
   "source": [
    "We can also do reverse transform:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7ea82f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "love =  0.05232698507085136\n"
     ]
    }
   ],
   "source": [
    "word_vector = {}\n",
    " \n",
    "word_vector['cat']  = .3*topic['petness'] +\\\n",
    "    .1*topic['animalness'] +\\\n",
    "    0*topic['cityness']\n",
    "\n",
    "word_vector['dog'] = .3*topic['petness'] +\\\n",
    "    .1*topic['animalness'] -\\\n",
    "    .1*topic['cityness']\n",
    " \n",
    "word_vector['apple'] = 0*topic['petness'] -\\\n",
    "    .1*topic['animalness'] +\\\n",
    "    .2*topic['cityness']\n",
    "\n",
    "word_vector['lion'] = 0*topic['petness'] +\\\n",
    "    .5*topic['animalness'] -\\\n",
    "    .1*topic['cityness']\n",
    "\n",
    "word_vector['NYC'] = -.2*topic['petness'] +\\\n",
    "    .1*topic['animalness'] +\\\n",
    "    .5*topic['cityness']\n",
    "\n",
    "word_vector['love'] = .2*topic['petness'] -\\\n",
    "    .1*topic['animalness'] +\\\n",
    "    .1*topic['cityness']\n",
    "\n",
    "print(\"love = \", word_vector['love'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c874a51d",
   "metadata": {},
   "source": [
    "By this we can see how much topic carries a given word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c0b0a5",
   "metadata": {},
   "source": [
    "<img src =\"3DWords.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a185889",
   "metadata": {},
   "source": [
    "We compressed 6D vector of words to 3D vector of topics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a4e9d59",
   "metadata": {},
   "source": [
    "**Question:** How to autmoatize topics finding and scoring sentences with respect to topics they represents? \n",
    "\n",
    "**Answer:** 'You shall know a word by the company it keeps.' J. R. Firth\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8d6512b",
   "metadata": {},
   "source": [
    "**Definition** LSA is an algorithm to analyze your TF-IDF matrix (table of TF-IDF vectors) to gather up words into topics. It works on bag-of-words vectors, too, but TF-IDF vectors give slightly better results. LSA also optimizes these topics to maintain diversity in the topic dimensions;\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c332a2fc",
   "metadata": {},
   "source": [
    "Possible LSA alghoritms:\n",
    "- SVD (Singular Value Deomposition)\n",
    "- PCA (Principal Components Analysis)\n",
    "\n",
    "Another related alghorimts:\n",
    "- LDA (Linear Discriminant Analysis)\n",
    "- LDiA (Latant Dirichlet Allocation)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebc7b16d",
   "metadata": {},
   "source": [
    "### A note on scikit-learn classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "333fca4f",
   "metadata": {},
   "source": [
    "Typical workflow of scikit-learn classifier:\n",
    "- create classifier\n",
    "- fit model - call fit() method\n",
    "- predict using fitted model - call predict() method\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebed245c",
   "metadata": {},
   "source": [
    "<img src = \"01_08.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc289a9",
   "metadata": {},
   "source": [
    "<img src=\"01_02.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950a8227",
   "metadata": {},
   "source": [
    "<img src=\"01_01.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "906ca39d",
   "metadata": {},
   "source": [
    "For more info about ML see:\n",
    "- S. Raschka, V. Mirjalili, 'Python Machine Learning', Packt Publishing 2nd edition 2017"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "494828e4",
   "metadata": {},
   "source": [
    "## LDA classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04043b66",
   "metadata": {},
   "source": [
    "[LDA](https://en.wikipedia.org/wiki/Linear_discriminant_analysis) is supervised learning alghoritm - you need class labels to train t.\n",
    "\n",
    "\n",
    "LDA training overiew:\n",
    "1. Compute the average position (centroid) of all the TF-IDF vectors within the class (such as spam SMS messages).\n",
    "2. Compute the average position (centroid) of all the TF-IDF vectors not in the class (such as nonspam SMS messages).\n",
    "3. Compute the vector difference between the centroids (the line that connects them).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b99656",
   "metadata": {},
   "source": [
    "#### Example - SPAM classifiaction using LDA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b723415a",
   "metadata": {},
   "source": [
    "We use a one-dimensional distincion of spammines - scalar value of spammines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4dc6a73b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len =  4837\n",
      "sum =  638\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.width = 120\n",
    "sms = pd.read_csv('sms-spam.csv')\n",
    "index = ['sms{}{}'.format(i, '!'*j) for (i,j) in zip(range(len(sms)), sms.spam)]\n",
    "sms = pd.DataFrame(sms.values, columns=sms.columns, index=index)\n",
    "sms['spam'] = sms.spam.astype(int)\n",
    "print(\"len = \", len(sms))\n",
    "print(\"spam msg sum = \", sms.spam.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "18ff1585",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>spam</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sms0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms2!</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms5!</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0  spam                                               text\n",
       "sms0           0     0  Go until jurong point, crazy.. Available only ...\n",
       "sms1           1     0                      Ok lar... Joking wif u oni...\n",
       "sms2!          2     1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "sms3           3     0  U dun say so early hor... U c already then say...\n",
       "sms4           4     0  Nah I don't think he goes to usf, he lives aro...\n",
       "sms5!          5     1  FreeMsg Hey there darling it's been 3 week's n..."
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.head(6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c1bdf59",
   "metadata": {},
   "source": [
    "Now we do tokenization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "835fcfa8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape tfidf =  (4837, 9232)\n",
      "spam msg sum =  638\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize.casual import casual_tokenize\n",
    "tfidf_model = TfidfVectorizer(tokenizer=casual_tokenize)\n",
    "tfidf_docs = tfidf_model.fit_transform(raw_documents=sms.text).toarray()\n",
    "print(\"shape tfidf = \", tfidf_docs.shape)\n",
    "print(\"spam msg sum = \", sms.spam.sum())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef5cf18",
   "metadata": {},
   "source": [
    "**Note** The nltk.casual_tokenizer gave you 9,232 words in your vocabulary. You have almost twice as many words as you have messages. And you have almost ten times as many words as spam messages. So your model won’t have a lot of information about the words that will indicate whether a message is spam or not. Usually, a Naive Bayes\n",
    "classifier won’t work well when your vocabulary is much larger than the number of labeled examples in your dataset. That’s where the semantic analysis techniques of this chapter can help.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ce3ac4f",
   "metadata": {},
   "source": [
    "Now we do LDA step by step (see [sklearn.discriminant_analysis.LinearDiscriminantAnalysis](https://scikit-learn.org/stable/modules/generated/sklearn.discriminant_analysis.LinearDiscriminantAnalysis.html) for black-box):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b2ac63d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "spam centroids =  [0.06 0.   0.   ... 0.   0.   0.  ]\n",
      "non-spam centroids =  [0.02 0.01 0.   ... 0.   0.   0.  ]\n"
     ]
    }
   ],
   "source": [
    "mask = sms.spam.astype(bool).values\n",
    "spam_centroid = tfidf_docs[mask].mean(axis=0)\n",
    "ham_centroid = tfidf_docs[~mask].mean(axis=0)\n",
    "print(\"spam centroids = \", spam_centroid.round(2))\n",
    "print(\"non-spam centroids = \", ham_centroid.round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ea8ee16",
   "metadata": {},
   "source": [
    "Project tfidf vector to the direction between centroids positions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "890c69ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vector between centroids =  [-0.01 -0.02  0.04 ... -0.01 -0.    0.  ]\n"
     ]
    }
   ],
   "source": [
    "spamminess_score = tfidf_docs.dot(spam_centroid - ham_centroid)\n",
    "print(\"vector between centroids = \", spamminess_score.round(2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3af9a047",
   "metadata": {},
   "source": [
    "<img src= \"SpamCloudPoints.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cb56c40",
   "metadata": {},
   "source": [
    "The arrow from the nonspam centroid to the spam centroid is the line that defines\n",
    "your trained model. You can see how some of the green dots are on the back side of\n",
    "the arrow, so you could get a negative spamminess score when you project them onto\n",
    "this line between the centroids.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48651a61",
   "metadata": {},
   "source": [
    "We can normalize 'spammines' to [0;1] interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "880d3164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>spam</th>\n",
       "      <th>lda_predict</th>\n",
       "      <th>lda_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sms0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.23</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms2!</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.29</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms5!</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms8!</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms9!</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.77</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms11!</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms12!</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.38</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms14</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0.55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms15!</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.53</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.28</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms19!</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.63</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        spam  lda_predict  lda_score\n",
       "sms0       0            0       0.23\n",
       "sms1       0            0       0.18\n",
       "sms2!      1            1       0.72\n",
       "sms3       0            0       0.18\n",
       "sms4       0            0       0.29\n",
       "sms5!      1            1       0.55\n",
       "sms6       0            0       0.32\n",
       "sms7       0            0       0.50\n",
       "sms8!      1            1       0.89\n",
       "sms9!      1            1       0.77\n",
       "sms10      0            0       0.24\n",
       "sms11!     1            1       0.79\n",
       "sms12!     1            1       0.92\n",
       "sms13      0            0       0.38\n",
       "sms14      0            1       0.55\n",
       "sms15!     1            1       0.53\n",
       "sms16      0            0       0.13\n",
       "sms17      0            0       0.25\n",
       "sms18      0            0       0.28\n",
       "sms19!     1            1       0.63"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "sms['lda_score'] = MinMaxScaler().fit_transform(spamminess_score.reshape(-1,1))\n",
    "sms['lda_predict'] = (sms.lda_score > .5).astype(int)\n",
    "sms['spam lda_predict lda_score'.split()].round(2).head(20)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1914a889",
   "metadata": {},
   "source": [
    "Notice that roughly values of lda_score above 0.5 are indication of potential spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "52c7f165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.977"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1. - (sms.spam - sms.lda_predict).abs().sum() / len(sms)).round(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b46bcea6",
   "metadata": {},
   "source": [
    "We got 97.7% of correct preditions!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2be5dec",
   "metadata": {},
   "source": [
    "Calulate confusion matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b9c59c2",
   "metadata": {},
   "source": [
    "<img src=\"ConfusionMatrix.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "9277821b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/dist-packages/pugnlp/stats.py:504: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  self.__setattr__('_hist_labels', self.sum().astype(int))\n",
      "/usr/local/lib/python3.8/dist-packages/pugnlp/stats.py:510: UserWarning: Pandas doesn't allow columns to be created via a new attribute name - see https://pandas.pydata.org/pandas-docs/stable/indexing.html#attribute-access\n",
      "  setattr(self, '_hist_classes', self.T.sum())\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>lda_predict</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>spam</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4135</td>\n",
       "      <td>64</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45</td>\n",
       "      <td>593</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "lda_predict     0    1\n",
       "spam                  \n",
       "0            4135   64\n",
       "1              45  593"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pugnlp.stats import Confusion\n",
    "Confusion(sms['spam lda_predict'.split()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19a5937e",
   "metadata": {},
   "source": [
    "## LSA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "735f6279",
   "metadata": {},
   "source": [
    "**Latent semantic analysis** is based on the oldest and most commonly-used technique for dimension reduction, singular value decomposition. SVD was in widespread use long before the term “machine learning” even existed. SVD decomposes a matrix into three square matrices, one of which is diagonal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "183909a0",
   "metadata": {},
   "source": [
    "Using SVD, LSA can break down your TF-IDF term-document matrix into three simpler matrices. And they can be multiplied back together to produce the original matrix, without any changes. This is like factorization of a large integer.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1165d8bd",
   "metadata": {},
   "source": [
    "<img src = \"1024px-Singular-Value-Decomposition.svg.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50639e5f",
   "metadata": {},
   "source": [
    "<img src = \"Singular_value_decomposition_visualisation.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6c2549d",
   "metadata": {},
   "source": [
    "### PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe4740c",
   "metadata": {},
   "source": [
    "One of the advanced LSA approach is to use [**Principal Component Analysis (PCA)**](https://en.wikipedia.org/wiki/Principal_component_analysis). \n",
    "\n",
    "\n",
    "The basic idea is to transform coordinates to the one that axis are defined by the eigenvectors of covariance matrix.\n",
    "\n",
    "**Remeber** the more variance, the more information!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739e54ff",
   "metadata": {},
   "source": [
    "One can consider PCA as SVD with improvements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa492d76",
   "metadata": {},
   "source": [
    "PCA is implemented in [Sciki Learn library](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dd7d6b",
   "metadata": {},
   "source": [
    "#### Example SMS spam subjects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381abcd3",
   "metadata": {},
   "source": [
    "Read data from csv:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "57ab8124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>spam</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sms0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Go until jurong point, crazy.. Available only ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms1</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar... Joking wif u oni...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms2!</th>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>Free entry in 2 a wkly comp to win FA Cup fina...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms3</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>U dun say so early hor... U c already then say...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms4</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>Nah I don't think he goes to usf, he lives aro...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms5!</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>FreeMsg Hey there darling it's been 3 week's n...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Unnamed: 0  spam                                               text\n",
       "sms0            0     0  Go until jurong point, crazy.. Available only ...\n",
       "sms1            1     0                      Ok lar... Joking wif u oni...\n",
       "sms2!           2     1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
       "sms3            3     0  U dun say so early hor... U c already then say...\n",
       "sms4            4     0  Nah I don't think he goes to usf, he lives aro...\n",
       "sms5!           5     1  FreeMsg Hey there darling it's been 3 week's n..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "pd.options.display.width = 120\n",
    "sms = pd.read_csv('sms-spam.csv')\n",
    "\n",
    "index = ['sms{}{}'.format(i, '!'*j) for (i,j) in zip(range(len(sms)), sms.spam)]\n",
    "sms.index = index\n",
    "sms.head(6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ca8733",
   "metadata": {},
   "source": [
    "Construct TF-IDF vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "344c0531",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9232"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize.casual import casual_tokenize\n",
    "tfidf = TfidfVectorizer(tokenizer=casual_tokenize)\n",
    "tfidf_docs = tfidf.fit_transform(raw_documents=sms.text).toarray()\n",
    "len(tfidf.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "687174e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4837, 9232)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_docs = pd.DataFrame(tfidf_docs)\n",
    "tfidf_docs = tfidf_docs - tfidf_docs.mean()\n",
    "tfidf_docs.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d1221c21",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "638"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.spam.sum()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dc06cc9",
   "metadata": {},
   "source": [
    "Do PCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e8ff1c80",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic0</th>\n",
       "      <th>topic1</th>\n",
       "      <th>topic2</th>\n",
       "      <th>topic3</th>\n",
       "      <th>topic4</th>\n",
       "      <th>topic5</th>\n",
       "      <th>topic6</th>\n",
       "      <th>topic7</th>\n",
       "      <th>topic8</th>\n",
       "      <th>topic9</th>\n",
       "      <th>topic10</th>\n",
       "      <th>topic11</th>\n",
       "      <th>topic12</th>\n",
       "      <th>topic13</th>\n",
       "      <th>topic14</th>\n",
       "      <th>topic15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sms0</th>\n",
       "      <td>0.201</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-0.065</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>0.010</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>-0.041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms1</th>\n",
       "      <td>0.404</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.025</td>\n",
       "      <td>-0.005</td>\n",
       "      <td>-0.028</td>\n",
       "      <td>0.046</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.049</td>\n",
       "      <td>0.045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms2!</th>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.090</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.091</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>0.053</td>\n",
       "      <td>0.123</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.023</td>\n",
       "      <td>-0.018</td>\n",
       "      <td>-0.049</td>\n",
       "      <td>-0.041</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms3</th>\n",
       "      <td>0.329</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.056</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>-0.073</td>\n",
       "      <td>0.062</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>0.024</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.066</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>0.034</td>\n",
       "      <td>0.054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms4</th>\n",
       "      <td>0.002</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.034</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>0.060</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.007</td>\n",
       "      <td>0.018</td>\n",
       "      <td>0.028</td>\n",
       "      <td>-0.082</td>\n",
       "      <td>0.029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms5!</th>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.122</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.167</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.040</td>\n",
       "      <td>-0.062</td>\n",
       "      <td>-0.030</td>\n",
       "      <td>0.074</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.034</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       topic0  topic1  topic2  topic3  topic4  topic5  topic6  topic7  topic8  topic9  topic10  topic11  topic12  \\\n",
       "sms0    0.201   0.003   0.037   0.011  -0.019  -0.053   0.039  -0.065   0.011  -0.082    0.010    0.007   -0.001   \n",
       "sms1    0.404  -0.094  -0.078   0.051   0.100   0.047   0.023   0.065   0.024  -0.025   -0.005   -0.028    0.046   \n",
       "sms2!  -0.030  -0.048   0.090  -0.067   0.091  -0.043  -0.000  -0.001  -0.057   0.053    0.123   -0.014    0.023   \n",
       "sms3    0.329  -0.033  -0.035  -0.016   0.052   0.056  -0.166  -0.073   0.062  -0.108    0.024   -0.014    0.066   \n",
       "sms4    0.002   0.031   0.038   0.034  -0.075  -0.093  -0.044   0.060  -0.046   0.029    0.031    0.007    0.018   \n",
       "sms5!  -0.016   0.059   0.014  -0.006   0.122  -0.040   0.005   0.167  -0.023   0.065    0.040   -0.062   -0.030   \n",
       "\n",
       "       topic13  topic14  topic15  \n",
       "sms0    -0.035   -0.006   -0.041  \n",
       "sms1    -0.016    0.049    0.045  \n",
       "sms2!   -0.018   -0.049   -0.041  \n",
       "sms3    -0.043    0.034    0.054  \n",
       "sms4     0.028   -0.082    0.029  \n",
       "sms5!    0.074    0.014   -0.034  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=16)\n",
    "pca = pca.fit(tfidf_docs)\n",
    "pca_topic_vectors = pca.transform(tfidf_docs)\n",
    "columns = ['topic{}'.format(i) for i in range(pca.n_components)]\n",
    "pca_topic_vectors = pd.DataFrame(pca_topic_vectors, columns=columns, index=index)\n",
    "pca_topic_vectors.round(3).head(6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b70e62d",
   "metadata": {},
   "source": [
    "**Note** We can find the weights of any fitted sklearn transformation by examining its .components_ attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1c44a64a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'go': 3807,\n",
       " 'until': 8487,\n",
       " 'jurong': 4675,\n",
       " 'point': 6296,\n",
       " ',': 13,\n",
       " 'crazy': 2549,\n",
       " '..': 21,\n",
       " 'available': 1531,\n",
       " 'only': 5910,\n",
       " 'in': 4396,\n",
       " 'bugis': 1973,\n",
       " 'n': 5594,\n",
       " 'great': 3894,\n",
       " 'world': 8977,\n",
       " 'la': 4811,\n",
       " 'e': 3056,\n",
       " 'buffet': 1971,\n",
       " '...': 25,\n",
       " 'cine': 2277,\n",
       " 'there': 8071,\n",
       " 'got': 3855,\n",
       " 'amore': 1296,\n",
       " 'wat': 8736,\n",
       " 'ok': 5874,\n",
       " 'lar': 4848,\n",
       " 'joking': 4642,\n",
       " 'wif': 8875,\n",
       " 'u': 8395,\n",
       " 'oni': 5906,\n",
       " 'free': 3604,\n",
       " 'entry': 3195,\n",
       " '2': 471,\n",
       " 'a': 1054,\n",
       " 'wkly': 8933,\n",
       " 'comp': 2386,\n",
       " 'to': 8192,\n",
       " 'win': 8890,\n",
       " 'fa': 3328,\n",
       " 'cup': 2608,\n",
       " 'final': 3450,\n",
       " 'tkts': 8180,\n",
       " '21st': 497,\n",
       " 'may': 5272,\n",
       " '2005': 487,\n",
       " '.': 15,\n",
       " 'text': 8020,\n",
       " '87121': 948,\n",
       " 'receive': 6688,\n",
       " 'question': 6574,\n",
       " '(': 9,\n",
       " 'std': 7651,\n",
       " 'txt': 8379,\n",
       " 'rate': 6628,\n",
       " ')': 10,\n",
       " 't': 7889,\n",
       " '&': 7,\n",
       " \"c's\": 2020,\n",
       " 'apply': 1383,\n",
       " '08452810075': 115,\n",
       " 'over': 6003,\n",
       " '18': 438,\n",
       " \"'\": 8,\n",
       " 's': 6959,\n",
       " 'dun': 3041,\n",
       " 'say': 7034,\n",
       " 'so': 7438,\n",
       " 'early': 3069,\n",
       " 'hor': 4207,\n",
       " 'c': 2019,\n",
       " 'already': 1268,\n",
       " 'then': 8065,\n",
       " 'nah': 5606,\n",
       " 'i': 4311,\n",
       " \"don't\": 2948,\n",
       " 'think': 8092,\n",
       " 'he': 4048,\n",
       " 'goes': 3819,\n",
       " 'usf': 8537,\n",
       " 'lives': 5004,\n",
       " 'around': 1435,\n",
       " 'here': 4104,\n",
       " 'though': 8111,\n",
       " 'freemsg': 3613,\n",
       " 'hey': 4116,\n",
       " 'darling': 2666,\n",
       " \"it's\": 4535,\n",
       " 'been': 1693,\n",
       " '3': 591,\n",
       " \"week's\": 8788,\n",
       " 'now': 5784,\n",
       " 'and': 1310,\n",
       " 'no': 5732,\n",
       " 'word': 8967,\n",
       " 'back': 1584,\n",
       " '!': 0,\n",
       " \"i'd\": 4312,\n",
       " 'like': 4954,\n",
       " 'some': 7454,\n",
       " 'fun': 3677,\n",
       " 'you': 9158,\n",
       " 'up': 8489,\n",
       " 'for': 3552,\n",
       " 'it': 4533,\n",
       " 'still': 7674,\n",
       " '?': 1037,\n",
       " 'tb': 7955,\n",
       " 'xxx': 9097,\n",
       " 'chgs': 2230,\n",
       " 'send': 7127,\n",
       " '£': 9216,\n",
       " '1.50': 344,\n",
       " 'rcv': 6641,\n",
       " 'even': 3240,\n",
       " 'my': 5584,\n",
       " 'brother': 1942,\n",
       " 'is': 4519,\n",
       " 'not': 5769,\n",
       " 'speak': 7529,\n",
       " 'with': 8918,\n",
       " 'me': 5281,\n",
       " 'they': 8083,\n",
       " 'treat': 8312,\n",
       " 'aids': 1214,\n",
       " 'patent': 6106,\n",
       " 'as': 1452,\n",
       " 'per': 6148,\n",
       " 'your': 9171,\n",
       " 'request': 6796,\n",
       " 'melle': 5315,\n",
       " 'oru': 5968,\n",
       " 'minnaminunginte': 5386,\n",
       " 'nurungu': 5807,\n",
       " 'vettam': 8599,\n",
       " 'has': 4022,\n",
       " 'set': 7154,\n",
       " 'callertune': 2047,\n",
       " 'all': 1253,\n",
       " 'callers': 2046,\n",
       " 'press': 6418,\n",
       " '*': 11,\n",
       " '9': 982,\n",
       " 'copy': 2489,\n",
       " 'friends': 3634,\n",
       " 'winner': 8900,\n",
       " 'valued': 8569,\n",
       " 'network': 5678,\n",
       " 'customer': 2620,\n",
       " 'have': 4036,\n",
       " 'selected': 7113,\n",
       " 'receivea': 6689,\n",
       " '900': 986,\n",
       " 'prize': 6450,\n",
       " 'reward': 6851,\n",
       " 'claim': 2283,\n",
       " 'call': 2038,\n",
       " '09061701461': 263,\n",
       " 'code': 2344,\n",
       " 'kl341': 4771,\n",
       " 'valid': 8565,\n",
       " '12': 384,\n",
       " 'hours': 4226,\n",
       " 'had': 3965,\n",
       " 'mobile': 5441,\n",
       " '11': 371,\n",
       " 'months': 5484,\n",
       " 'or': 5946,\n",
       " 'more': 5489,\n",
       " 'r': 6590,\n",
       " 'entitled': 3192,\n",
       " 'update': 8495,\n",
       " 'the': 8052,\n",
       " 'latest': 4862,\n",
       " 'colour': 2364,\n",
       " 'mobiles': 5442,\n",
       " 'camera': 2058,\n",
       " 'co': 2333,\n",
       " 'on': 5897,\n",
       " '08002986030': 99,\n",
       " \"i'm\": 4314,\n",
       " 'gonna': 3834,\n",
       " 'be': 1669,\n",
       " 'home': 4176,\n",
       " 'soon': 7483,\n",
       " 'want': 8715,\n",
       " 'talk': 7921,\n",
       " 'about': 1076,\n",
       " 'this': 8100,\n",
       " 'stuff': 7741,\n",
       " 'anymore': 1350,\n",
       " 'tonight': 8235,\n",
       " 'k': 4683,\n",
       " \"i've\": 4316,\n",
       " 'cried': 2566,\n",
       " 'enough': 3182,\n",
       " 'today': 8199,\n",
       " 'six': 7341,\n",
       " 'chances': 2172,\n",
       " 'cash': 2116,\n",
       " 'from': 3652,\n",
       " '100': 354,\n",
       " '20,000': 482,\n",
       " 'pounds': 6357,\n",
       " '>': 1035,\n",
       " 'csh': 2584,\n",
       " '87575': 952,\n",
       " 'cost': 2501,\n",
       " '150p': 415,\n",
       " '/': 27,\n",
       " 'day': 2683,\n",
       " '6days': 827,\n",
       " '16': 431,\n",
       " '+': 12,\n",
       " 'tsandcs': 8344,\n",
       " 'reply': 6788,\n",
       " 'hl': 4148,\n",
       " '4': 659,\n",
       " 'info': 4433,\n",
       " 'urgent': 8513,\n",
       " 'won': 8950,\n",
       " '1': 337,\n",
       " 'week': 8787,\n",
       " 'membership': 5321,\n",
       " 'our': 5980,\n",
       " '100,000': 355,\n",
       " 'jackpot': 4564,\n",
       " ':': 1006,\n",
       " '81010': 900,\n",
       " 'www.dbuk.net': 9039,\n",
       " 'lccltd': 4880,\n",
       " 'pobox': 6286,\n",
       " '4403ldnw1a7rw18': 696,\n",
       " 'searching': 7081,\n",
       " 'right': 6863,\n",
       " 'words': 8968,\n",
       " 'thank': 8037,\n",
       " 'breather': 1912,\n",
       " 'promise': 6487,\n",
       " 'wont': 8958,\n",
       " 'take': 7913,\n",
       " 'help': 4089,\n",
       " 'granted': 3883,\n",
       " 'will': 8887,\n",
       " 'fulfil': 3673,\n",
       " 'wonderful': 8955,\n",
       " 'blessing': 1802,\n",
       " 'at': 1488,\n",
       " 'times': 8158,\n",
       " 'date': 2675,\n",
       " 'sunday': 7805,\n",
       " 'xxxmobilemovieclub': 9098,\n",
       " 'use': 8531,\n",
       " 'credit': 2556,\n",
       " 'click': 2306,\n",
       " 'wap': 8719,\n",
       " 'link': 4977,\n",
       " 'next': 5696,\n",
       " 'message': 5340,\n",
       " 'http://wap': 4259,\n",
       " 'xxxmobilemovieclub.com': 9099,\n",
       " '=': 1031,\n",
       " 'qjkgighjjgcbl': 6566,\n",
       " 'oh': 5869,\n",
       " 'watching': 8743,\n",
       " ':)': 1008,\n",
       " 'eh': 3116,\n",
       " 'remember': 6755,\n",
       " 'how': 4233,\n",
       " 'spell': 7545,\n",
       " 'his': 4139,\n",
       " 'name': 5612,\n",
       " 'yes': 9137,\n",
       " 'did': 2823,\n",
       " 'v': 8553,\n",
       " 'naughty': 5635,\n",
       " 'make': 5193,\n",
       " 'wet': 8828,\n",
       " 'fine': 3458,\n",
       " 'if': 4350,\n",
       " 'that': 8045,\n",
       " '\\x92': 9211,\n",
       " 'way': 8753,\n",
       " 'feel': 3400,\n",
       " 'its': 4546,\n",
       " 'gota': 3856,\n",
       " 'b': 1560,\n",
       " 'england': 3173,\n",
       " 'macedonia': 5156,\n",
       " '-': 14,\n",
       " 'dont': 2952,\n",
       " 'miss': 5402,\n",
       " 'goals': 3812,\n",
       " 'team': 7968,\n",
       " 'news': 5692,\n",
       " 'ur': 8510,\n",
       " 'national': 5629,\n",
       " '87077': 947,\n",
       " 'eg': 3109,\n",
       " 'try': 8340,\n",
       " 'wales': 8695,\n",
       " 'scotland': 7060,\n",
       " '4txt': 737,\n",
       " 'ú1': 9220,\n",
       " '20': 481,\n",
       " 'poboxox': 6287,\n",
       " '36504w45wq': 629,\n",
       " 'seriously': 7147,\n",
       " '‘': 9225,\n",
       " 'm': 5139,\n",
       " 'going': 3823,\n",
       " 'ha': 3961,\n",
       " 'ü': 9221,\n",
       " 'pay': 6119,\n",
       " 'first': 3476,\n",
       " 'when': 8840,\n",
       " 'da': 2639,\n",
       " 'stock': 7678,\n",
       " 'comin': 2376,\n",
       " 'aft': 1182,\n",
       " 'finish': 3462,\n",
       " 'lunch': 5121,\n",
       " 'str': 7702,\n",
       " 'down': 2974,\n",
       " 'lor': 5058,\n",
       " 'ard': 1410,\n",
       " 'smth': 7422,\n",
       " 'ffffffffff': 3420,\n",
       " 'alright': 1269,\n",
       " 'can': 2062,\n",
       " 'meet': 5303,\n",
       " 'sooner': 7485,\n",
       " 'just': 4677,\n",
       " 'forced': 3554,\n",
       " 'myself': 5591,\n",
       " 'eat': 3081,\n",
       " 'slice': 7372,\n",
       " 'really': 6670,\n",
       " 'hungry': 4287,\n",
       " 'tho': 8107,\n",
       " 'sucks': 7778,\n",
       " 'mark': 5230,\n",
       " 'getting': 3767,\n",
       " 'worried': 8981,\n",
       " 'knows': 4782,\n",
       " 'sick': 7289,\n",
       " 'turn': 8362,\n",
       " 'pizza': 6238,\n",
       " 'lol': 5035,\n",
       " 'always': 1279,\n",
       " 'convincing': 2476,\n",
       " 'catch': 2128,\n",
       " 'bus': 1993,\n",
       " 'are': 1411,\n",
       " 'frying': 3660,\n",
       " 'an': 1305,\n",
       " 'egg': 3111,\n",
       " 'tea': 7962,\n",
       " 'eating': 3084,\n",
       " \"mom's\": 5463,\n",
       " 'left': 4901,\n",
       " 'dinner': 2858,\n",
       " 'do': 2909,\n",
       " 'love': 5081,\n",
       " \"we're\": 8760,\n",
       " 'packing': 6032,\n",
       " 'car': 2085,\n",
       " \"i'll\": 4313,\n",
       " 'let': 4923,\n",
       " 'know': 4779,\n",
       " \"there's\": 8074,\n",
       " 'room': 6906,\n",
       " 'ahhh': 1209,\n",
       " 'work': 8970,\n",
       " 'vaguely': 8560,\n",
       " 'what': 8832,\n",
       " 'does': 2923,\n",
       " 'wait': 8689,\n",
       " \"that's\": 8048,\n",
       " 'clear': 2300,\n",
       " 'were': 8817,\n",
       " 'sure': 7832,\n",
       " 'being': 1713,\n",
       " 'sarcastic': 7010,\n",
       " 'why': 8868,\n",
       " 'x': 9076,\n",
       " \"doesn't\": 2926,\n",
       " 'live': 5000,\n",
       " 'us': 8525,\n",
       " 'yeah': 9125,\n",
       " 'was': 8728,\n",
       " 'apologetic': 1371,\n",
       " 'fallen': 3352,\n",
       " 'out': 5983,\n",
       " 'she': 7199,\n",
       " 'actin': 1123,\n",
       " 'spoilt': 7572,\n",
       " 'child': 2238,\n",
       " 'caught': 2132,\n",
       " 'till': 8152,\n",
       " 'but': 1999,\n",
       " 'we': 8757,\n",
       " \"won't\": 8951,\n",
       " 'doing': 2938,\n",
       " 'too': 8242,\n",
       " 'badly': 1589,\n",
       " 'cheers': 2214,\n",
       " 'tell': 7985,\n",
       " 'anything': 1356,\n",
       " 'fear': 3391,\n",
       " 'of': 5847,\n",
       " 'fainting': 3344,\n",
       " 'housework': 4231,\n",
       " 'quick': 6577,\n",
       " 'cuppa': 2610,\n",
       " 'thanks': 8038,\n",
       " 'subscription': 7766,\n",
       " 'ringtone': 6872,\n",
       " 'uk': 8415,\n",
       " 'charged': 2184,\n",
       " '5': 745,\n",
       " 'month': 5479,\n",
       " 'please': 6265,\n",
       " 'confirm': 2431,\n",
       " 'by': 2016,\n",
       " 'replying': 6790,\n",
       " 'yup': 9192,\n",
       " 'look': 5046,\n",
       " 'timings': 8162,\n",
       " 'msg': 5528,\n",
       " 'again': 1190,\n",
       " 'xuhui': 9093,\n",
       " 'learn': 4892,\n",
       " '2nd': 567,\n",
       " 'her': 4099,\n",
       " 'lesson': 4921,\n",
       " '8am': 974,\n",
       " 'oops': 5921,\n",
       " \"roommate's\": 6909,\n",
       " 'done': 2950,\n",
       " 'see': 7098,\n",
       " 'letter': 4926,\n",
       " 'decide': 2711,\n",
       " 'hello': 4084,\n",
       " \"how's\": 4235,\n",
       " 'saturday': 7024,\n",
       " 'texting': 8027,\n",
       " \"you'd\": 9159,\n",
       " 'decided': 2712,\n",
       " 'tomo': 8224,\n",
       " 'trying': 8342,\n",
       " 'invite': 4493,\n",
       " 'pls': 6273,\n",
       " 'ahead': 1208,\n",
       " 'watts': 8751,\n",
       " 'wanted': 8716,\n",
       " 'weekend': 8791,\n",
       " 'abiola': 1072,\n",
       " 'forget': 3560,\n",
       " 'need': 5654,\n",
       " 'crave': 2546,\n",
       " 'most': 5499,\n",
       " 'sweet': 7863,\n",
       " 'arabian': 1407,\n",
       " 'steed': 7658,\n",
       " 'mmmmmm': 5431,\n",
       " 'yummy': 9187,\n",
       " '07732584351': 62,\n",
       " 'rodger': 6895,\n",
       " 'burns': 1990,\n",
       " 'tried': 8321,\n",
       " 're': 6645,\n",
       " 'sms': 7416,\n",
       " 'nokia': 5744,\n",
       " 'camcorder': 2056,\n",
       " '08000930705': 95,\n",
       " 'delivery': 2750,\n",
       " 'tomorrow': 8226,\n",
       " 'who': 8859,\n",
       " 'seeing': 7101,\n",
       " 'hope': 4198,\n",
       " 'man': 5203,\n",
       " 'well': 8807,\n",
       " 'endowed': 3163,\n",
       " 'am': 1281,\n",
       " '<#>': 1024,\n",
       " 'inches': 4401,\n",
       " 'calls': 2053,\n",
       " 'messages': 5344,\n",
       " 'missed': 5405,\n",
       " \"didn't\": 2828,\n",
       " 'get': 3760,\n",
       " 'hep': 4098,\n",
       " 'immunisation': 4379,\n",
       " 'nigeria': 5708,\n",
       " 'fair': 3345,\n",
       " 'hopefully': 4201,\n",
       " 'tyler': 8389,\n",
       " \"can't\": 2063,\n",
       " 'could': 2511,\n",
       " 'maybe': 5274,\n",
       " 'ask': 1463,\n",
       " 'bit': 1779,\n",
       " 'stubborn': 7730,\n",
       " 'hospital': 4214,\n",
       " 'kept': 4730,\n",
       " 'telling': 7986,\n",
       " 'weak': 8762,\n",
       " 'sucker': 7776,\n",
       " 'hospitals': 4215,\n",
       " 'suckers': 7777,\n",
       " 'thinked': 8093,\n",
       " 'time': 8154,\n",
       " 'saw': 7033,\n",
       " 'class': 2292,\n",
       " 'gram': 3875,\n",
       " 'usually': 8543,\n",
       " 'runs': 6949,\n",
       " 'half': 3977,\n",
       " 'eighth': 3119,\n",
       " 'smarter': 7395,\n",
       " 'gets': 3763,\n",
       " 'almost': 1264,\n",
       " 'whole': 8862,\n",
       " 'second': 7085,\n",
       " 'fyi': 3693,\n",
       " 'ride': 6862,\n",
       " 'morning': 5493,\n",
       " \"he's\": 4050,\n",
       " 'crashing': 2545,\n",
       " 'place': 6240,\n",
       " 'wow': 8997,\n",
       " 'never': 5683,\n",
       " 'realized': 6668,\n",
       " 'embarassed': 3144,\n",
       " 'accomodations': 1103,\n",
       " 'thought': 8112,\n",
       " 'liked': 4955,\n",
       " 'since': 7314,\n",
       " 'best': 1733,\n",
       " 'seemed': 7105,\n",
       " 'happy': 4011,\n",
       " '\"': 1,\n",
       " 'cave': 2136,\n",
       " 'sorry': 7494,\n",
       " 'give': 3788,\n",
       " 'offered': 5855,\n",
       " 'embarassing': 3145,\n",
       " 'ac': 1089,\n",
       " 'sptv': 7594,\n",
       " 'new': 5687,\n",
       " 'jersey': 4608,\n",
       " 'devils': 2803,\n",
       " 'detroit': 2797,\n",
       " 'red': 6711,\n",
       " 'wings': 8898,\n",
       " 'play': 6255,\n",
       " 'ice': 4330,\n",
       " 'hockey': 4161,\n",
       " 'correct': 2494,\n",
       " 'incorrect': 4412,\n",
       " 'end': 3158,\n",
       " 'mallika': 5202,\n",
       " 'sherawat': 7208,\n",
       " 'yesterday': 9141,\n",
       " 'find': 3455,\n",
       " '@': 1038,\n",
       " '<url>': 1030,\n",
       " 'congrats': 2437,\n",
       " 'year': 9126,\n",
       " 'special': 7531,\n",
       " 'cinema': 2278,\n",
       " 'pass': 6094,\n",
       " 'yours': 9176,\n",
       " '09061209465': 258,\n",
       " 'suprman': 7830,\n",
       " 'matrix': 5263,\n",
       " 'starwars': 7638,\n",
       " 'etc': 3230,\n",
       " 'bx420': 2014,\n",
       " 'ip4': 4502,\n",
       " '5we': 781,\n",
       " '150pm': 417,\n",
       " 'later': 4861,\n",
       " 'meeting': 5305,\n",
       " 'where': 8846,\n",
       " 'reached': 6652,\n",
       " 'gauti': 3728,\n",
       " 'sehwag': 7110,\n",
       " 'odi': 5846,\n",
       " 'series': 7145,\n",
       " 'pick': 6211,\n",
       " '$': 5,\n",
       " 'burger': 1985,\n",
       " 'yourself': 9177,\n",
       " 'move': 5513,\n",
       " 'pain': 6039,\n",
       " 'killing': 4754,\n",
       " 'good': 3836,\n",
       " 'joke': 4636,\n",
       " 'girls': 3785,\n",
       " 'situation': 7338,\n",
       " 'seekers': 7102,\n",
       " 'part': 6081,\n",
       " 'checking': 2208,\n",
       " 'iq': 4508,\n",
       " 'roommates': 6910,\n",
       " 'took': 8245,\n",
       " 'forever': 3557,\n",
       " 'come': 2371,\n",
       " 'double': 2966,\n",
       " 'check': 2204,\n",
       " 'hair': 3972,\n",
       " 'dresser': 2998,\n",
       " 'said': 6980,\n",
       " 'wun': 9024,\n",
       " 'cut': 2624,\n",
       " 'short': 7248,\n",
       " 'nice': 5701,\n",
       " 'pleased': 6266,\n",
       " 'advise': 1165,\n",
       " 'following': 3534,\n",
       " 'recent': 6692,\n",
       " 'review': 6849,\n",
       " 'mob': 5439,\n",
       " 'awarded': 1549,\n",
       " '1500': 414,\n",
       " 'bonus': 1844,\n",
       " '09066364589': 306,\n",
       " 'song': 7478,\n",
       " 'dedicated': 2722,\n",
       " 'which': 8853,\n",
       " 'dedicate': 2721,\n",
       " 'valuable': 8566,\n",
       " 'frnds': 3643,\n",
       " 'rply': 6925,\n",
       " 'complimentary': 2406,\n",
       " 'trip': 8322,\n",
       " 'eurodisinc': 3234,\n",
       " 'trav': 8304,\n",
       " 'aco': 1119,\n",
       " '41': 679,\n",
       " '1000': 356,\n",
       " 'dis': 2871,\n",
       " '6': 785,\n",
       " 'morefrmmob': 5490,\n",
       " 'shracomorsglsuplt': 7273,\n",
       " '10': 350,\n",
       " 'ls1': 5103,\n",
       " '3aj': 638,\n",
       " 'hear': 4062,\n",
       " 'divorce': 2900,\n",
       " 'barbie': 1620,\n",
       " 'comes': 2373,\n",
       " \"ken's\": 4728,\n",
       " 'plane': 6247,\n",
       " 'wah': 8682,\n",
       " 'lucky': 5114,\n",
       " 'save': 7029,\n",
       " 'money': 5470,\n",
       " 'hee': 4075,\n",
       " 'finished': 3464,\n",
       " 'hi': 4120,\n",
       " 'babe': 1574,\n",
       " 'im': 4368,\n",
       " 'wanna': 8713,\n",
       " 'something': 7464,\n",
       " 'xx': 9094,\n",
       " 'performed': 6155,\n",
       " 'waiting': 8692,\n",
       " 'machan': 5158,\n",
       " 'once': 5901,\n",
       " 'thats': 8051,\n",
       " 'cool': 2481,\n",
       " 'gentleman': 3751,\n",
       " 'dignity': 2848,\n",
       " 'respect': 6816,\n",
       " 'peoples': 6147,\n",
       " 'very': 8598,\n",
       " 'much': 5544,\n",
       " 'shy': 7283,\n",
       " 'pa': 6027,\n",
       " 'operate': 5928,\n",
       " 'after': 1183,\n",
       " 'same': 6996,\n",
       " 'looking': 5050,\n",
       " 'job': 4623,\n",
       " \"ta's\": 7896,\n",
       " 'earn': 3070,\n",
       " 'ah': 1204,\n",
       " 'stop': 7688,\n",
       " 'urgnt': 8517,\n",
       " 'real': 6662,\n",
       " 'yo': 9152,\n",
       " 'tickets': 8142,\n",
       " 'one': 5903,\n",
       " 'jacket': 4563,\n",
       " 'used': 8532,\n",
       " 'multis': 5553,\n",
       " 'started': 7632,\n",
       " 'requests': 6797,\n",
       " 'came': 2057,\n",
       " 'bed': 1686,\n",
       " 'coins': 2350,\n",
       " 'factory': 3335,\n",
       " 'gotta': 3860,\n",
       " 'nitros': 5727,\n",
       " 'ela': 3124,\n",
       " 'kano': 4708,\n",
       " 'il': 4362,\n",
       " 'download': 2975,\n",
       " 'wen': 8811,\n",
       " 'don': 2947,\n",
       " 'stand': 7620,\n",
       " 'close': 2313,\n",
       " 'll': 5008,\n",
       " 'another': 1332,\n",
       " 'night': 5710,\n",
       " 'spent': 7550,\n",
       " 'late': 4858,\n",
       " 'afternoon': 1185,\n",
       " 'casualty': 2126,\n",
       " 'means': 5291,\n",
       " \"haven't\": 4039,\n",
       " 'any': 1346,\n",
       " 'y': 9107,\n",
       " '42moro': 689,\n",
       " 'includes': 4405,\n",
       " 'sheets': 7203,\n",
       " 'smile': 7403,\n",
       " 'pleasure': 6268,\n",
       " 'trouble': 8328,\n",
       " 'pours': 6359,\n",
       " 'rain': 6602,\n",
       " 'sum': 7798,\n",
       " 'hurts': 4297,\n",
       " 'becoz': 1684,\n",
       " 'someone': 7457,\n",
       " 'loves': 5090,\n",
       " 'smiling': 7407,\n",
       " 'service': 7150,\n",
       " 'representative': 6794,\n",
       " '0800 169 6031': 86,\n",
       " 'between': 1741,\n",
       " '10am': 365,\n",
       " '9pm': 1002,\n",
       " 'guaranteed': 3930,\n",
       " '5000': 755,\n",
       " 'havent': 4040,\n",
       " 'planning': 6251,\n",
       " 'buy': 2004,\n",
       " 'lido': 4937,\n",
       " '530': 767,\n",
       " 'show': 7264,\n",
       " 'collected': 2358,\n",
       " 'simply': 7311,\n",
       " 'password': 6102,\n",
       " 'mix': 5421,\n",
       " '85069': 934,\n",
       " 'verify': 8594,\n",
       " 'usher': 8538,\n",
       " 'britney': 1932,\n",
       " 'fml': 3525,\n",
       " 'po': 6284,\n",
       " 'box': 1879,\n",
       " '5249': 764,\n",
       " 'mk17': 5424,\n",
       " '92h': 990,\n",
       " '450ppw': 705,\n",
       " 'telugu': 7991,\n",
       " 'movie': 5516,\n",
       " 'abt': 1084,\n",
       " 'loads': 5014,\n",
       " 'loans': 5016,\n",
       " 'wk': 8928,\n",
       " 'hols': 4174,\n",
       " 'run': 6946,\n",
       " 'forgot': 3565,\n",
       " 'hairdressers': 3974,\n",
       " 'appointment': 1386,\n",
       " 'four': 3584,\n",
       " 'shower': 7266,\n",
       " 'beforehand': 1702,\n",
       " 'cause': 2133,\n",
       " 'prob': 6456,\n",
       " 'coffee': 2345,\n",
       " 'animation': 1319,\n",
       " 'nothing': 5774,\n",
       " 'else': 3138,\n",
       " 'okay': 5877,\n",
       " 'price': 6431,\n",
       " 'long': 5042,\n",
       " 'legal': 4904,\n",
       " 'them': 8061,\n",
       " 'ave': 1536,\n",
       " 'ams': 1301,\n",
       " 'gone': 3832,\n",
       " '4the': 735,\n",
       " 'driving': 3007,\n",
       " 'test': 8014,\n",
       " 'yet': 9142,\n",
       " \"you're\": 9162,\n",
       " 'mean': 5287,\n",
       " 'guess': 3936,\n",
       " 'gave': 3729,\n",
       " 'boston': 1866,\n",
       " 'men': 5326,\n",
       " 'changed': 2174,\n",
       " 'search': 7080,\n",
       " 'location': 5019,\n",
       " 'nyc': 5819,\n",
       " 'cuz': 2631,\n",
       " 'signin': 7299,\n",
       " 'page': 6035,\n",
       " 'says': 7038,\n",
       " 'umma': 8423,\n",
       " 'life': 4940,\n",
       " 'vava': 8580,\n",
       " 'lot': 5066,\n",
       " 'dear': 2699,\n",
       " 'wishes': 8912,\n",
       " 'birthday': 1777,\n",
       " 'making': 5197,\n",
       " 'truly': 8335,\n",
       " 'memorable': 5323,\n",
       " 'aight': 1216,\n",
       " 'hit': 4141,\n",
       " 'would': 8993,\n",
       " 'ip': 4501,\n",
       " 'address': 1141,\n",
       " 'considering': 2449,\n",
       " 'computer': 2412,\n",
       " \"isn't\": 4528,\n",
       " 'minecraft': 5380,\n",
       " 'server': 7149,\n",
       " 'grumpy': 3923,\n",
       " 'old': 5889,\n",
       " 'people': 6146,\n",
       " 'mom': 5462,\n",
       " 'better': 1738,\n",
       " 'lying': 5135,\n",
       " 'jokes': 4640,\n",
       " 'worry': 8983,\n",
       " 'busy': 1998,\n",
       " 'plural': 6277,\n",
       " 'noun': 5781,\n",
       " 'research': 6802,\n",
       " 'dinner.msg': 2859,\n",
       " 'cos': 2499,\n",
       " 'things': 8091,\n",
       " 'scared': 7044,\n",
       " 'mah': 5180,\n",
       " 'loud': 5076,\n",
       " 'gent': 3749,\n",
       " 'contact': 2454,\n",
       " 'last': 4855,\n",
       " 'weekends': 8793,\n",
       " 'draw': 2989,\n",
       " 'shows': 7272,\n",
       " '09064012160': 282,\n",
       " 'k52': 4691,\n",
       " '12hrs': 398,\n",
       " '150ppm': 419,\n",
       " 'wa': 8677,\n",
       " 'openin': 5925,\n",
       " 'sentence': 7138,\n",
       " 'formal': 3569,\n",
       " 'anyway': 1360,\n",
       " 'juz': 4682,\n",
       " 'tt': 8348,\n",
       " 'eatin': 3083,\n",
       " 'puttin': 6554,\n",
       " 'weight': 8798,\n",
       " 'haha': 3968,\n",
       " 'anythin': 1355,\n",
       " 'happened': 4003,\n",
       " 'entered': 3185,\n",
       " 'cabin': 2025,\n",
       " \"b'day\": 1562,\n",
       " 'boss': 1865,\n",
       " 'felt': 3411,\n",
       " 'askd': 1464,\n",
       " 'invited': 4494,\n",
       " 'apartment': 1365,\n",
       " 'went': 8814,\n",
       " 'specially': 7536,\n",
       " 'holiday': 4171,\n",
       " 'flights': 3502,\n",
       " 'inc': 4399,\n",
       " 'operator': 5929,\n",
       " '08712778109': 166,\n",
       " '10p': 368,\n",
       " 'min': 5372,\n",
       " 'goodo': 3846,\n",
       " 'must': 5575,\n",
       " 'friday': 3626,\n",
       " 'egg-potato': 3112,\n",
       " 'ratio': 6631,\n",
       " 'tortilla': 8262,\n",
       " 'needed': 5656,\n",
       " 'hmm': 4153,\n",
       " 'uncle': 8433,\n",
       " 'informed': 4438,\n",
       " 'paying': 6124,\n",
       " 'school': 7050,\n",
       " 'directly': 2865,\n",
       " 'food': 3542,\n",
       " 'private': 6447,\n",
       " '2004': 486,\n",
       " 'account': 1107,\n",
       " 'statement': 7641,\n",
       " '07742676969': 64,\n",
       " '786': 864,\n",
       " 'unredeemed': 8477,\n",
       " 'points': 6297,\n",
       " '08719180248': 213,\n",
       " 'identifier': 4344,\n",
       " '45239': 707,\n",
       " 'expires': 3307,\n",
       " '2000': 484,\n",
       " 'caller': 2045,\n",
       " '5/9': 752,\n",
       " '03': 46,\n",
       " 'landline': 4835,\n",
       " '09064019788': 288,\n",
       " '42wr29c': 690,\n",
       " 'apples': 1381,\n",
       " 'pairs': 6044,\n",
       " 'malarky': 5199,\n",
       " 'todays': 8205,\n",
       " 'voda': 8645,\n",
       " 'numbers': 5804,\n",
       " 'ending': 3160,\n",
       " '7548': 856,\n",
       " '350': 624,\n",
       " 'award': 1548,\n",
       " 'match': 5251,\n",
       " '08712300220': 149,\n",
       " 'quoting': 6589,\n",
       " '4041': 674,\n",
       " 'standard': 7621,\n",
       " 'rates': 6629,\n",
       " 'app': 1375,\n",
       " 'sao': 7004,\n",
       " 'mu': 5542,\n",
       " 'predict': 6392,\n",
       " \"ü'll\": 9222,\n",
       " 'buying': 2007,\n",
       " 'yetunde': 9144,\n",
       " \"hasn't\": 4024,\n",
       " 'sent': 7137,\n",
       " 'bother': 1869,\n",
       " 'sending': 7129,\n",
       " 'involve': 4498,\n",
       " \"shouldn't\": 7259,\n",
       " 'imposed': 4386,\n",
       " 'apologise': 1372,\n",
       " 'girl': 3782,\n",
       " 'del': 2740,\n",
       " 'bak': 1597,\n",
       " 'lucyxx': 5118,\n",
       " 'tmorrow.pls': 8185,\n",
       " 'accomodate': 1102,\n",
       " 'answer': 1335,\n",
       " 'sunshine': 7812,\n",
       " 'quiz': 6584,\n",
       " 'q': 6559,\n",
       " 'top': 8253,\n",
       " 'sony': 7480,\n",
       " 'dvd': 3051,\n",
       " 'player': 6257,\n",
       " 'country': 2518,\n",
       " 'algarve': 1245,\n",
       " 'ansr': 1334,\n",
       " '82277': 907,\n",
       " 'sp': 7516,\n",
       " 'tyrone': 8394,\n",
       " 'laid': 4827,\n",
       " 'dogging': 2932,\n",
       " 'locations': 5020,\n",
       " 'direct': 2864,\n",
       " 'join': 4631,\n",
       " \"uk's\": 8416,\n",
       " 'largest': 4852,\n",
       " 'bt': 1958,\n",
       " 'txting': 8383,\n",
       " 'gravel': 3888,\n",
       " '69888': 822,\n",
       " 'nt': 5791,\n",
       " 'ec2a': 3086,\n",
       " '31p': 611,\n",
       " '@150p': 1039,\n",
       " 'haf': 3967,\n",
       " 'msn': 5534,\n",
       " 'yijue@hotmail.com': 9149,\n",
       " 'him': 4132,\n",
       " 'rooms': 6911,\n",
       " 'befor': 1699,\n",
       " 'activities': 1129,\n",
       " \"you'll\": 9161,\n",
       " 'msgs': 5533,\n",
       " 'chat': 2195,\n",
       " ...}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " tfidf.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f35e220b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('!',\n",
       " '\"',\n",
       " '#',\n",
       " '#150',\n",
       " '#5000',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '. .',\n",
       " '. . .',\n",
       " '. . . .',\n",
       " '. . . . .',\n",
       " '. ..',\n",
       " '..',\n",
       " '.. .',\n",
       " '.. . . .',\n",
       " '.. ... ...',\n",
       " '...',\n",
       " '... . . . .',\n",
       " '/',\n",
       " '0',\n",
       " '00',\n",
       " '00870405040',\n",
       " '0089',\n",
       " '01',\n",
       " '0121 2025050',\n",
       " '01223585236',\n",
       " '01223585334',\n",
       " '01256987',\n",
       " '02',\n",
       " '02/06',\n",
       " '02/09',\n",
       " '0207 153 9153',\n",
       " '0207 153 9996',\n",
       " '0207-083-6089',\n",
       " '02072069400',\n",
       " '02073162414',\n",
       " '02085076972',\n",
       " '03',\n",
       " '03530150',\n",
       " '04',\n",
       " '04/09',\n",
       " '05',\n",
       " '050703',\n",
       " '06',\n",
       " '06.05',\n",
       " '06/11',\n",
       " '07/11',\n",
       " '07008009200',\n",
       " '07046744435',\n",
       " '07090201529',\n",
       " '07090298926',\n",
       " '07099833605',\n",
       " '07123456789',\n",
       " '07732584351',\n",
       " '07734396839',\n",
       " '07742676969',\n",
       " '07753741225',\n",
       " '0776xxxxxxx',\n",
       " '07786200117',\n",
       " '077xxx',\n",
       " '078',\n",
       " '07801543489',\n",
       " '07808',\n",
       " '07808247860',\n",
       " '07808726822',\n",
       " '07815296484',\n",
       " '07821230901',\n",
       " '078498',\n",
       " '07880867867',\n",
       " '0789xxxxxxx',\n",
       " '07946746291',\n",
       " '0796xxxxxx',\n",
       " '07973788240',\n",
       " '07xxxxxxxxx',\n",
       " '08',\n",
       " '0800',\n",
       " '0800 0721072',\n",
       " '0800 169 6031',\n",
       " '0800 195 6669',\n",
       " '0800 1956669',\n",
       " '0800 5050',\n",
       " '0800 542 0578',\n",
       " '0800 542 0825',\n",
       " '08000407165',\n",
       " '08000776320',\n",
       " '08000839402',\n",
       " '08000930705',\n",
       " '08000938767',\n",
       " '08001950382',\n",
       " '08002888812',\n",
       " '08002986030',\n",
       " '08002986906',\n",
       " '08002988890',\n",
       " '08006344447',\n",
       " '0808 145 4742',\n",
       " '08081263000',\n",
       " '08081560665',\n",
       " '0819',\n",
       " '0844',\n",
       " '08448350055',\n",
       " '08448714184',\n",
       " '0845 021 3680',\n",
       " '0845 2814032',\n",
       " '08450542832',\n",
       " '08452810071',\n",
       " '08452810073',\n",
       " '08452810075',\n",
       " '0870',\n",
       " '08700435505',\n",
       " '08700469649',\n",
       " '08700621170',\n",
       " '08701213186',\n",
       " '08701237397',\n",
       " '08701417012',\n",
       " '08701624',\n",
       " '08701752560',\n",
       " '08701872873',\n",
       " '08702411827',\n",
       " '08702490080',\n",
       " '08702840625',\n",
       " '08702840625.comuk',\n",
       " '08704050406',\n",
       " '08704439680',\n",
       " '08706091795',\n",
       " '08707379102',\n",
       " '08707500020',\n",
       " '08707509020',\n",
       " '08707533310',\n",
       " '08707808226',\n",
       " '08708034412',\n",
       " '08708800282',\n",
       " '08709222922',\n",
       " '08709501522',\n",
       " '0871-4719',\n",
       " '0871-872-9755',\n",
       " '0871-872-9758',\n",
       " '08710471114',\n",
       " '08712101358',\n",
       " '08712103738',\n",
       " '08712120250',\n",
       " '08712300220',\n",
       " '08712317606',\n",
       " '08712400200',\n",
       " '08712400602',\n",
       " '08712400603',\n",
       " '08712402050',\n",
       " '08712402578',\n",
       " '08712402779',\n",
       " '08712402902',\n",
       " '08712402972',\n",
       " '08712404000',\n",
       " '08712405020',\n",
       " '08712405022',\n",
       " '08712460324',\n",
       " '08712466669',\n",
       " '08712778107',\n",
       " '08712778108',\n",
       " '08712778109',\n",
       " '08714342399',\n",
       " '08714712377',\n",
       " '08714712379',\n",
       " '08714712388',\n",
       " '08714712394',\n",
       " '08714712412',\n",
       " '08714714011',\n",
       " '08714740323',\n",
       " '08714742804',\n",
       " '08715203028',\n",
       " '08715203649',\n",
       " '08715203652',\n",
       " '08715203656',\n",
       " '08715203677',\n",
       " '08715203685',\n",
       " '08715203694',\n",
       " '08715205273',\n",
       " '08715500022',\n",
       " '08715705022',\n",
       " '08717111821',\n",
       " '08717168528',\n",
       " '08717205546',\n",
       " '0871750',\n",
       " '08717507382',\n",
       " '08717509990',\n",
       " '08717890890',\n",
       " '08717895698',\n",
       " '08717898035',\n",
       " '08718711108',\n",
       " '08718720201',\n",
       " '08718723815',\n",
       " '08718725756',\n",
       " '08718726270',\n",
       " '08718726970',\n",
       " '08718726971',\n",
       " '08718726978',\n",
       " '08718727200',\n",
       " '08718727868',\n",
       " '08718727870',\n",
       " '08718728876',\n",
       " '08718730555',\n",
       " '08718730666',\n",
       " '08718738001',\n",
       " '08718738002',\n",
       " '08718738034',\n",
       " '08719180219',\n",
       " '08719180248',\n",
       " '08719181259',\n",
       " '08719181503',\n",
       " '08719181513',\n",
       " '08719839835',\n",
       " '08719899217',\n",
       " '08719899229',\n",
       " '08719899230',\n",
       " '09',\n",
       " '09041940223',\n",
       " '09050000301',\n",
       " '09050000327',\n",
       " '09050000332',\n",
       " '09050000460',\n",
       " '09050000555',\n",
       " '09050000878',\n",
       " '09050000928',\n",
       " '09050001295',\n",
       " '09050001808',\n",
       " '09050002311',\n",
       " '09050003091',\n",
       " '09050005321',\n",
       " '09050090044',\n",
       " '09050280520',\n",
       " '09053750005',\n",
       " '09056242159',\n",
       " '09057039994',\n",
       " '09058091854',\n",
       " '09058091870',\n",
       " '09058094454',\n",
       " '09058094455',\n",
       " '09058094507',\n",
       " '09058094565',\n",
       " '09058094583',\n",
       " '09058094594',\n",
       " '09058094597',\n",
       " '09058094599',\n",
       " '09058095107',\n",
       " '09058095201',\n",
       " '09058097189',\n",
       " '09058097218',\n",
       " '09058098002',\n",
       " '09058099801',\n",
       " '09061104276',\n",
       " '09061104283',\n",
       " '09061209465',\n",
       " '09061213237',\n",
       " '09061221061',\n",
       " '09061221066',\n",
       " '09061701444',\n",
       " '09061701461',\n",
       " '09061701851',\n",
       " '09061701939',\n",
       " '09061702893',\n",
       " '09061743386',\n",
       " '09061743806',\n",
       " '09061743810',\n",
       " '09061743811',\n",
       " '09061744553',\n",
       " '09061749602',\n",
       " '09061790121',\n",
       " '09061790125',\n",
       " '09061790126',\n",
       " '09063440451',\n",
       " '09063442151',\n",
       " '09063458130',\n",
       " '09063463',\n",
       " '09064011000',\n",
       " '09064012103',\n",
       " '09064012160',\n",
       " '09064015307',\n",
       " '09064017295',\n",
       " '09064017305',\n",
       " '09064018838',\n",
       " '09064019014',\n",
       " '09064019788',\n",
       " '09065069120',\n",
       " '09065069154',\n",
       " '09065171142',\n",
       " '09065174042',\n",
       " '09065394514',\n",
       " '09065394973',\n",
       " '09065989180',\n",
       " '09065989182',\n",
       " '09066350750',\n",
       " '09066358152',\n",
       " '09066358361',\n",
       " '09066361921',\n",
       " '09066362206',\n",
       " '09066362220',\n",
       " '09066362231',\n",
       " '09066364311',\n",
       " '09066364349',\n",
       " '09066364589',\n",
       " '09066368327',\n",
       " '09066368470',\n",
       " '09066368753',\n",
       " '09066380611',\n",
       " '09066382422',\n",
       " '09066612661',\n",
       " '09066649731',\n",
       " '09066660100',\n",
       " '09071512432',\n",
       " '09071512433',\n",
       " '09071517866',\n",
       " '09077818151',\n",
       " '09090204448',\n",
       " '09090900040',\n",
       " '09094100151',\n",
       " '09094646631',\n",
       " '09094646899',\n",
       " '09095350301',\n",
       " '09096102316',\n",
       " '09099725823',\n",
       " '09099726395',\n",
       " '09099726429',\n",
       " '09099726481',\n",
       " '09099726553',\n",
       " '09111030116',\n",
       " '09111032124',\n",
       " '09701213186',\n",
       " '0a',\n",
       " '0p',\n",
       " '0quit',\n",
       " '1',\n",
       " '1,000',\n",
       " '1,2',\n",
       " '1,50',\n",
       " '1,500',\n",
       " '1.20',\n",
       " '1.5',\n",
       " '1.50',\n",
       " '1.childish',\n",
       " '1/08',\n",
       " '1/1',\n",
       " '1/2',\n",
       " '1/3',\n",
       " '10',\n",
       " '10,000',\n",
       " '10.1',\n",
       " '10/06',\n",
       " '100',\n",
       " '100,000',\n",
       " '1000',\n",
       " '1000call',\n",
       " '1000s',\n",
       " '100p',\n",
       " '100txt',\n",
       " '1013',\n",
       " '1030',\n",
       " '10:10',\n",
       " '10:30',\n",
       " '10am',\n",
       " '10k',\n",
       " '10mins',\n",
       " '10p',\n",
       " '10ppm',\n",
       " '10th',\n",
       " '11',\n",
       " '11.48',\n",
       " '1120',\n",
       " '113',\n",
       " '1131',\n",
       " '114/14',\n",
       " '1146',\n",
       " '1151',\n",
       " '116',\n",
       " '1172',\n",
       " '118p',\n",
       " '11mths',\n",
       " '11pm',\n",
       " '12',\n",
       " '12,000',\n",
       " '1205',\n",
       " '120p',\n",
       " '121',\n",
       " '1225',\n",
       " '123',\n",
       " '125',\n",
       " '1250',\n",
       " '125gift',\n",
       " '128',\n",
       " '1282essexcm61xn',\n",
       " '12:30',\n",
       " '12hours',\n",
       " '12hrs',\n",
       " '12mths',\n",
       " '12n146tf15',\n",
       " '12n146tf150p',\n",
       " '13/10',\n",
       " '13/4',\n",
       " '130',\n",
       " '1327',\n",
       " '139',\n",
       " '140',\n",
       " '1405',\n",
       " '140ppm',\n",
       " '1450',\n",
       " '146tf150p',\n",
       " '14thmarch',\n",
       " '150',\n",
       " '1500',\n",
       " '150p',\n",
       " '150p16',\n",
       " '150pm',\n",
       " '150ppermesssubscription',\n",
       " '150ppm',\n",
       " '150ppmmobilesvary',\n",
       " '150ppmpobox10183bhamb64xe',\n",
       " '150ppmsg',\n",
       " '150ppmx3age16',\n",
       " '150pw',\n",
       " '150x3',\n",
       " '151',\n",
       " '1510',\n",
       " '15541',\n",
       " '15:26',\n",
       " '15h',\n",
       " '16',\n",
       " '16.150',\n",
       " '165',\n",
       " '1680',\n",
       " '16yrs',\n",
       " '177',\n",
       " '177hp51fl',\n",
       " '18',\n",
       " '18/11',\n",
       " '180',\n",
       " '1843',\n",
       " '1896wc1n3xx',\n",
       " '18:0430-',\n",
       " '18p',\n",
       " '18yrs',\n",
       " '1apple',\n",
       " '1b6a5ecef91ff9',\n",
       " '1cup',\n",
       " '1da',\n",
       " '1er',\n",
       " '1hr',\n",
       " '1im',\n",
       " '1lemon',\n",
       " '1million',\n",
       " '1more',\n",
       " '1n3xx',\n",
       " '1pm',\n",
       " '1st',\n",
       " '1st4terms',\n",
       " '1stchoice.co.uk',\n",
       " '1stone',\n",
       " '1thing',\n",
       " '1tulsi',\n",
       " '1win150ppmx3',\n",
       " '1win150ppmx3age16',\n",
       " '1win150ppmx3age16subscription',\n",
       " '1winaweek',\n",
       " '1winawk',\n",
       " '1x150p',\n",
       " '1yf',\n",
       " '2',\n",
       " '2,000',\n",
       " '2-4-',\n",
       " '2.15',\n",
       " '2.30',\n",
       " '2.50',\n",
       " '2.im',\n",
       " '2.naughty',\n",
       " '2/2',\n",
       " '2/3',\n",
       " '20',\n",
       " '20,000',\n",
       " '200',\n",
       " '2000',\n",
       " '2003',\n",
       " '2004',\n",
       " '2005',\n",
       " '2006',\n",
       " '2007',\n",
       " '200p',\n",
       " '202',\n",
       " '20m12aq',\n",
       " '20p',\n",
       " '21',\n",
       " '21/11',\n",
       " '2187000',\n",
       " '21st',\n",
       " '22',\n",
       " '220',\n",
       " '220cm2',\n",
       " '23',\n",
       " '2309',\n",
       " '23f',\n",
       " '23g',\n",
       " '24',\n",
       " '24/10',\n",
       " '24/7',\n",
       " '245c2150pm',\n",
       " '24hrs',\n",
       " '24m',\n",
       " '24th',\n",
       " '25',\n",
       " '250',\n",
       " '250k',\n",
       " '255',\n",
       " '25p',\n",
       " '26.03',\n",
       " '26/10',\n",
       " '26/11',\n",
       " '2667',\n",
       " '26th',\n",
       " '27/03',\n",
       " '27/6',\n",
       " '28',\n",
       " '28/5',\n",
       " '28days',\n",
       " '28th',\n",
       " '28thfeb',\n",
       " '29',\n",
       " '29/03',\n",
       " '29/10',\n",
       " '2b',\n",
       " '2bed',\n",
       " '2bold',\n",
       " '2bremoved',\n",
       " '2c',\n",
       " '2channel',\n",
       " '2come',\n",
       " '2day',\n",
       " '2day.love',\n",
       " '2die',\n",
       " '2docd.please',\n",
       " '2end',\n",
       " '2exit',\n",
       " '2ez',\n",
       " '2find',\n",
       " '2getha',\n",
       " '2geva',\n",
       " '2go',\n",
       " '2go.did',\n",
       " '2gthr',\n",
       " '2hear',\n",
       " '2hook',\n",
       " '2hrs',\n",
       " '2i',\n",
       " '2kbsubject',\n",
       " '2marrow',\n",
       " '2mobile',\n",
       " '2moro',\n",
       " '2morow',\n",
       " '2morro',\n",
       " '2morrow',\n",
       " '2morrowxxxx',\n",
       " '2mro',\n",
       " '2mrw',\n",
       " '2mwen',\n",
       " '2nd',\n",
       " '2nhite',\n",
       " '2nights',\n",
       " '2nite',\n",
       " '2optout',\n",
       " '2p',\n",
       " '2px',\n",
       " '2rcv',\n",
       " '2stop',\n",
       " '2stoptx',\n",
       " '2stoptxt',\n",
       " '2tell',\n",
       " '2the',\n",
       " '2u',\n",
       " '2u2',\n",
       " '2watershd',\n",
       " '2waxsto',\n",
       " '2wks',\n",
       " '2worzels',\n",
       " '2wt',\n",
       " '2wu',\n",
       " '2years',\n",
       " '2yr',\n",
       " '2yrs',\n",
       " '3',\n",
       " '3.00',\n",
       " '3.75',\n",
       " '3.99',\n",
       " '3.sentiment',\n",
       " '30',\n",
       " '300',\n",
       " '3000',\n",
       " '300603',\n",
       " '300603t',\n",
       " '300p',\n",
       " '3030',\n",
       " '30apr',\n",
       " '30pp',\n",
       " '30s',\n",
       " '30th',\n",
       " '31',\n",
       " '31/10',\n",
       " '3100',\n",
       " '310303',\n",
       " '31p',\n",
       " '32',\n",
       " '32000',\n",
       " '3230',\n",
       " '32323',\n",
       " '326',\n",
       " '33.65',\n",
       " '330',\n",
       " '334',\n",
       " '334sk38ch',\n",
       " '3355',\n",
       " '33:50',\n",
       " '342/2',\n",
       " '350',\n",
       " '3510i',\n",
       " '35p',\n",
       " '3650',\n",
       " '36504',\n",
       " '36504w45wq',\n",
       " '365o4w45wq',\n",
       " '373',\n",
       " '3750',\n",
       " '37819',\n",
       " '38',\n",
       " '385',\n",
       " '391784',\n",
       " '39822',\n",
       " '3aj',\n",
       " '3cktz8r7',\n",
       " '3d',\n",
       " '3days',\n",
       " '3g',\n",
       " '3gbp',\n",
       " '3hrs',\n",
       " '3lions',\n",
       " '3lp',\n",
       " '3miles',\n",
       " '3mins',\n",
       " '3mobile',\n",
       " '3optical',\n",
       " '3pound',\n",
       " '3qxj9',\n",
       " '3rd',\n",
       " '3ss',\n",
       " '3uz',\n",
       " '3wks',\n",
       " '3x',\n",
       " '3xx',\n",
       " '4',\n",
       " '4-6',\n",
       " '4-7',\n",
       " '4.15',\n",
       " '4.30',\n",
       " '4.47',\n",
       " '4.49',\n",
       " '4.50',\n",
       " '4.cook',\n",
       " '4.rowdy',\n",
       " '40',\n",
       " '400',\n",
       " '400mins',\n",
       " '402',\n",
       " '403',\n",
       " '4041',\n",
       " '40411',\n",
       " '40533',\n",
       " '40gb',\n",
       " '40mph',\n",
       " '41',\n",
       " '41685',\n",
       " '41782',\n",
       " '420',\n",
       " '42049',\n",
       " '4217',\n",
       " '4235wc1n3xx',\n",
       " '42478',\n",
       " '42810',\n",
       " '4284',\n",
       " '42moro',\n",
       " '42wr29c',\n",
       " '430',\n",
       " '434',\n",
       " '434sk38wp150ppm18',\n",
       " '44',\n",
       " '440',\n",
       " '4403ldnw1a7rw18',\n",
       " '4477977060',\n",
       " '4478012592',\n",
       " '4487124040',\n",
       " '4490500003',\n",
       " '4490715124',\n",
       " '45',\n",
       " '450',\n",
       " '450p',\n",
       " '450ppw',\n",
       " '450pw',\n",
       " '45239',\n",
       " '45po139wa',\n",
       " '45w2tg150p',\n",
       " '47',\n",
       " '48',\n",
       " '4882',\n",
       " '48922',\n",
       " '49557',\n",
       " '4a',\n",
       " '4an18th',\n",
       " '4brekkie',\n",
       " '4d',\n",
       " '4eva',\n",
       " '4few',\n",
       " '4fil',\n",
       " '4get',\n",
       " '4get2text',\n",
       " '4give',\n",
       " '4got',\n",
       " '4goten',\n",
       " '4info',\n",
       " '4jx',\n",
       " '4msgs',\n",
       " '4mths',\n",
       " '4my',\n",
       " '4qf2',\n",
       " '4t',\n",
       " '4th',\n",
       " '4the',\n",
       " '4thnov.behind',\n",
       " '4txt',\n",
       " '4u',\n",
       " '4utxt',\n",
       " '4w',\n",
       " '4ward',\n",
       " '4wrd',\n",
       " '4xx26',\n",
       " '4years',\n",
       " '5',\n",
       " '5.00',\n",
       " '5.15',\n",
       " '5.30',\n",
       " '5.ful',\n",
       " '5.gardener',\n",
       " '5.terror',\n",
       " '5/9',\n",
       " '50',\n",
       " '500',\n",
       " '5000',\n",
       " '5000.00',\n",
       " '50award',\n",
       " '50p',\n",
       " '50s',\n",
       " '5120',\n",
       " '515',\n",
       " '5226',\n",
       " '523',\n",
       " '5249',\n",
       " '526',\n",
       " '528',\n",
       " '530',\n",
       " '54',\n",
       " '545',\n",
       " '5digital',\n",
       " '5free',\n",
       " '5ish',\n",
       " '5k',\n",
       " '5min',\n",
       " '5mls',\n",
       " '5p',\n",
       " '5pm',\n",
       " '5th',\n",
       " '5times',\n",
       " '5wb',\n",
       " '5we',\n",
       " '5wkg',\n",
       " '5wq',\n",
       " '5years',\n",
       " '6',\n",
       " '6.30',\n",
       " '6.45',\n",
       " '6.cruel',\n",
       " '6.house',\n",
       " '6.romantic',\n",
       " '60',\n",
       " '60,400',\n",
       " '600',\n",
       " '60p',\n",
       " '61',\n",
       " '61200',\n",
       " '61610',\n",
       " '62220cncl',\n",
       " '6230',\n",
       " '62468',\n",
       " '62735',\n",
       " '630',\n",
       " '63miles',\n",
       " '645',\n",
       " '65,61',\n",
       " '650',\n",
       " '66,382',\n",
       " '6600',\n",
       " '6650',\n",
       " '674',\n",
       " '6744123',\n",
       " '68866',\n",
       " '69',\n",
       " '69101',\n",
       " '69200',\n",
       " '69669',\n",
       " '69696',\n",
       " '69698',\n",
       " '69855',\n",
       " '69866.18',\n",
       " '69876',\n",
       " '69888',\n",
       " '69888nyt',\n",
       " '69911',\n",
       " '69969',\n",
       " '69988',\n",
       " '6days',\n",
       " '6gbp',\n",
       " '6hl',\n",
       " '6hrs',\n",
       " '6ish',\n",
       " '6missed',\n",
       " '6months',\n",
       " '6ph',\n",
       " '6pm',\n",
       " '6th',\n",
       " '6times',\n",
       " '6wu',\n",
       " '6zf',\n",
       " '7',\n",
       " '7.30',\n",
       " '7.8',\n",
       " '7.children',\n",
       " '7.romantic',\n",
       " '7.shy',\n",
       " '700',\n",
       " '71',\n",
       " '7250',\n",
       " '7250i',\n",
       " '730',\n",
       " '731',\n",
       " '734ls27yf',\n",
       " '74355',\n",
       " '75,000',\n",
       " '750',\n",
       " '7548',\n",
       " '75ldns7',\n",
       " '762',\n",
       " '7634',\n",
       " '7684',\n",
       " '77.11',\n",
       " '7732584351',\n",
       " '78',\n",
       " '786',\n",
       " '7876150',\n",
       " '79',\n",
       " '7:30',\n",
       " '7am',\n",
       " '7cfca1a',\n",
       " '7ish',\n",
       " '7oz',\n",
       " '7pm',\n",
       " '7th',\n",
       " '7ws',\n",
       " '7zs',\n",
       " '8',\n",
       " '8,22',\n",
       " '8-8',\n",
       " '8.30',\n",
       " '8.attractive',\n",
       " '8.lovable',\n",
       " '8.neighbour',\n",
       " '80',\n",
       " '800',\n",
       " '8000930705',\n",
       " '80062',\n",
       " '8007',\n",
       " '80082',\n",
       " '80086',\n",
       " '8012230',\n",
       " '80155',\n",
       " '80160',\n",
       " '80182',\n",
       " '8027',\n",
       " '80488',\n",
       " '80488.biz',\n",
       " '80608',\n",
       " '8077',\n",
       " '80878',\n",
       " '81010',\n",
       " '81151',\n",
       " '81303',\n",
       " '81618',\n",
       " '820554ad0a1705572711',\n",
       " '82228',\n",
       " '82242',\n",
       " '82277',\n",
       " '82277.unsub',\n",
       " '82324',\n",
       " '82468',\n",
       " '83021',\n",
       " '83039',\n",
       " '83049',\n",
       " '83110',\n",
       " '83118',\n",
       " '83222',\n",
       " '83332.please',\n",
       " '83338',\n",
       " '83355',\n",
       " '83370',\n",
       " '83383',\n",
       " '83435',\n",
       " '83600',\n",
       " '83738',\n",
       " '84',\n",
       " '84025',\n",
       " '84122',\n",
       " '84128',\n",
       " '84199',\n",
       " '84484',\n",
       " '85',\n",
       " '850',\n",
       " '85023',\n",
       " '85069',\n",
       " '85222',\n",
       " '85233',\n",
       " '8552',\n",
       " '85555',\n",
       " '86021',\n",
       " '861',\n",
       " '864233',\n",
       " '86688',\n",
       " '86888',\n",
       " '87021',\n",
       " '87066',\n",
       " '87070',\n",
       " '87077',\n",
       " '87121',\n",
       " '87131',\n",
       " '8714714',\n",
       " '87239',\n",
       " '87575',\n",
       " '8800',\n",
       " '88039',\n",
       " '88039.skilgme',\n",
       " '88066',\n",
       " '88088',\n",
       " '88222',\n",
       " '88600',\n",
       " '88800',\n",
       " '8883',\n",
       " '88877',\n",
       " '88888',\n",
       " '89',\n",
       " '89034',\n",
       " '89070',\n",
       " '89080',\n",
       " '89105',\n",
       " '89123',\n",
       " '89545',\n",
       " '89555',\n",
       " '89693',\n",
       " '89938',\n",
       " '8am',\n",
       " '8ball',\n",
       " '8i',\n",
       " '8lb',\n",
       " '8p',\n",
       " '8r',\n",
       " '8th',\n",
       " '8wp',\n",
       " '9',\n",
       " '9-6',\n",
       " '9.decent',\n",
       " '9.funny',\n",
       " '900',\n",
       " '9061100010',\n",
       " '910',\n",
       " '9280114',\n",
       " '92h',\n",
       " '930',\n",
       " '9307622',\n",
       " '945',\n",
       " '946',\n",
       " '95',\n",
       " '95qu',\n",
       " '97n7qp',\n",
       " '9832156',\n",
       " '9ae',\n",
       " ...)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_nums, terms = zip(*sorted(zip(tfidf.vocabulary_.values(), tfidf.vocabulary_.keys())))\n",
    "terms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ffa06d8c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>!</th>\n",
       "      <th>\"</th>\n",
       "      <th>#</th>\n",
       "      <th>#150</th>\n",
       "      <th>...</th>\n",
       "      <th>…</th>\n",
       "      <th>┾</th>\n",
       "      <th>〨ud</th>\n",
       "      <th>鈥</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>topic0</th>\n",
       "      <td>-0.071</td>\n",
       "      <td>0.008</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.002</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic1</th>\n",
       "      <td>0.063</td>\n",
       "      <td>0.008</td>\n",
       "      <td>0.000</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic2</th>\n",
       "      <td>0.071</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.001</td>\n",
       "      <td>...</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic3</th>\n",
       "      <td>-0.059</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "      <td>0.001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 9232 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            !      \"      #   #150  ...      …      ┾    〨ud      鈥\n",
       "topic0 -0.071  0.008 -0.001 -0.000  ... -0.002  0.001  0.001  0.001\n",
       "topic1  0.063  0.008  0.000 -0.000  ...  0.003  0.001  0.001  0.001\n",
       "topic2  0.071  0.027  0.000  0.001  ...  0.002 -0.001 -0.001 -0.001\n",
       "topic3 -0.059 -0.033 -0.001 -0.000  ...  0.001  0.001  0.001  0.001\n",
       "\n",
       "[4 rows x 9232 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = pd.DataFrame(pca.components_, columns=terms, index=['topic{}'.format(i) for i in range(16)])\n",
    "pd.options.display.max_columns = 8\n",
    "weights.head(4).round(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73fa3906",
   "metadata": {},
   "source": [
    "Now we can see which term contributes to each topic - we restrict to meaningful words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "eab353f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>!</th>\n",
       "      <th>;)</th>\n",
       "      <th>:)</th>\n",
       "      <th>half</th>\n",
       "      <th>off</th>\n",
       "      <th>free</th>\n",
       "      <th>crazy</th>\n",
       "      <th>deal</th>\n",
       "      <th>only</th>\n",
       "      <th>$</th>\n",
       "      <th>80</th>\n",
       "      <th>%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>topic0</th>\n",
       "      <td>-7.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-2.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic1</th>\n",
       "      <td>6.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>-2.3</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-3.8</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic2</th>\n",
       "      <td>7.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic3</th>\n",
       "      <td>-5.9</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-7.1</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-2.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic4</th>\n",
       "      <td>38.0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-12.4</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>9.9</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic5</th>\n",
       "      <td>-26.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-1.6</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.7</td>\n",
       "      <td>-1.4</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-1.8</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic6</th>\n",
       "      <td>-10.9</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>19.9</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-1.4</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic7</th>\n",
       "      <td>16.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-18.0</td>\n",
       "      <td>0.8</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-2.9</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-2.0</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic8</th>\n",
       "      <td>34.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5.0</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>3.2</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic9</th>\n",
       "      <td>7.5</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>16.3</td>\n",
       "      <td>1.5</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>6.3</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>3.1</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic10</th>\n",
       "      <td>-31.7</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-10.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>12.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic11</th>\n",
       "      <td>-24.8</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-27.0</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-1.4</td>\n",
       "      <td>4.8</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.6</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic12</th>\n",
       "      <td>-22.1</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>38.8</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-3.8</td>\n",
       "      <td>-0.5</td>\n",
       "      <td>0.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic13</th>\n",
       "      <td>12.9</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>32.4</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.5</td>\n",
       "      <td>5.8</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-2.2</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic14</th>\n",
       "      <td>3.7</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>13.7</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.9</td>\n",
       "      <td>4.2</td>\n",
       "      <td>0.2</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>3.7</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>topic15</th>\n",
       "      <td>-6.6</td>\n",
       "      <td>0.6</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>4.1</td>\n",
       "      <td>0.7</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-1.8</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            !   ;)    :)  half  off  free  crazy  deal  only    $   80    %\n",
       "topic0   -7.1  0.1  -0.5  -0.0 -0.4  -2.0   -0.0  -0.1  -2.2  0.3 -0.0 -0.0\n",
       "topic1    6.3  0.0   7.4   0.1  0.4  -2.3   -0.2  -0.1  -3.8 -0.1 -0.0 -0.2\n",
       "topic2    7.1  0.2  -0.1   0.0  0.3   4.4    0.1  -0.1   0.7  0.0  0.0  0.1\n",
       "topic3   -5.9 -0.3  -7.1   0.2  0.3  -0.2    0.0   0.1  -2.3  0.1 -0.1 -0.3\n",
       "topic4   38.0 -0.1 -12.4  -0.1 -0.2   9.9    0.1  -0.2   3.0  0.3  0.1 -0.1\n",
       "topic5  -26.5  0.1  -1.6  -0.3 -0.7  -1.4   -0.6  -0.2  -1.8 -0.9  0.0  0.0\n",
       "topic6  -10.9 -0.5  19.9  -0.4 -0.9  -0.6   -0.2  -0.1  -1.4 -0.0 -0.0 -0.1\n",
       "topic7   16.2  0.1 -18.0   0.8  0.8  -2.9    0.0   0.1  -2.0 -0.3  0.0 -0.1\n",
       "topic8   34.2  0.1   5.0  -0.4 -0.5   0.3   -0.4  -0.4   3.2 -0.6 -0.0 -0.2\n",
       "topic9    7.5 -0.3  16.3   1.5 -0.9   6.3   -0.5  -0.4   3.1 -0.4 -0.0  0.0\n",
       "topic10 -31.7 -0.2 -10.2   0.1  0.1  12.5    0.2   0.0   0.5 -0.1 -0.1 -0.2\n",
       "topic11 -24.8 -0.4 -27.0  -0.4 -1.4   4.8   -0.2  -0.1  -0.6  0.5  0.0  0.4\n",
       "topic12 -22.1 -0.2  38.8  -0.1  0.3  -3.8   -0.5   0.1   3.8  0.3  0.0  0.3\n",
       "topic13  12.9 -0.2  32.4  -0.2  0.5   5.8    0.4   0.1  -2.2 -0.3  0.0 -0.2\n",
       "topic14   3.7 -0.0  13.7  -0.4 -0.9   4.2    0.2  -0.1   3.7 -0.0  0.0 -0.3\n",
       "topic15  -6.6  0.6   1.7   0.5  1.5   4.1    0.7  -0.4  -1.8  0.6 -0.1  0.2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.options.display.max_columns = 12\n",
    "deals = weights['! ;) :) half off free crazy deal only $ 80 %'.split()].round(3) * 100\n",
    "deals\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cfc2ad5",
   "metadata": {},
   "source": [
    "**Note** The casual_tokenize tokenizer splits \"80%\" into [\"80\",\"%\"] and \"$80 million\" into [\"$\", 80\", \"million\"]. So unless you use LSA or a 2-gram tokenizer, your NLP pipeline wouldn’t notice the difference between 80% and $80 million. They’d both share the token “80.”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fe9eaf0d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "topic0    -11.9\n",
       "topic1      7.5\n",
       "topic2     12.7\n",
       "topic3    -15.5\n",
       "topic4     38.3\n",
       "topic5    -33.9\n",
       "topic6      4.8\n",
       "topic7     -5.3\n",
       "topic8     40.3\n",
       "topic9     32.2\n",
       "topic10   -29.1\n",
       "topic11   -49.2\n",
       "topic12    16.9\n",
       "topic13    49.0\n",
       "topic14    23.8\n",
       "topic15     1.0\n",
       "dtype: float64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "deals.T.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bafedfe",
   "metadata": {},
   "source": [
    "## Yet another PCA semantic analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32246c8",
   "metadata": {},
   "source": [
    "Prepare TF-IDF vector and then using PCA create semantic vector. Finally,we apply PCA to find interesting relations. The code will be more extensive and shows state-of-art NLP codes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6e9788c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import casual_tokenize\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# from nltk.stem import PorterStemmer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "\n",
    "\n",
    "NUM_TOPICS = 3\n",
    "NUM_WORDS = 6\n",
    "NUM_DOCS = NUM_PRETTY = 16\n",
    "SAVE_SORTED_CORPUS = ''  # 'cats_and_dogs_sorted.txt'\n",
    "# import nltk\n",
    "# nltk.download('wordnet')  # noqa\n",
    "# from nltk.stem.wordnet import WordNetLemmatizer\n",
    "\n",
    "\n",
    "# STOPWORDS = 'a an and or the do are with from for of on in by if at to into them'.split()\n",
    "# STOPWORDS += 'to at it its it\\'s that than our you your - -- \" \\' ? , . !'.split()\n",
    "STOPWORDS = []\n",
    "\n",
    "# SYNONYMS = dict(zip(\n",
    "#     'wolv people person women woman man human he  we  her she him his hers'.split(),\n",
    "#     'wolf her    her    her   her   her her   her her her her her her her'.split()))\n",
    "# SYNONYMS.update(dict(zip(\n",
    "#     'ate pat smarter have had isn\\'t hasn\\'t no  got get become been was were wa be sat seat sit'.split(),\n",
    "#     'eat pet smart   has  has not    not     not has has is     is   is  is   is is sit sit  sit'.split())))\n",
    "# SYNONYMS.update(dict(zip(\n",
    "#     'i me my mine our ours catbird bird birds birder tortoise turtle turtles turtle\\'s don\\'t'.split(),\n",
    "#     'i i  i  i    i   i    bird    bird birds bird   turtle   turtle turtle  turtle    not'.split())))\n",
    "SYNONYMS = {}\n",
    "\n",
    "stemmer = None  # PorterStemmer()\n",
    "\n",
    "pd.options.display.width = 110\n",
    "pd.options.display.max_columns = 14\n",
    "pd.options.display.max_colwidth = 32\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def normalize_corpus_words(corpus, stemmer=stemmer, synonyms=SYNONYMS, stopwords=STOPWORDS):\n",
    "    docs = [doc.lower() for doc in corpus]\n",
    "    docs = [casual_tokenize(doc) for doc in docs]\n",
    "    docs = [[synonyms.get(w, w) for w in words if w not in stopwords] for words in docs]\n",
    "    if stemmer:\n",
    "        docs = [[stemmer.stem(w) for w in words if w not in stopwords] for words in docs]\n",
    "        docs = [[synonyms.get(w, w) for w in words if w not in stopwords] for words in docs]\n",
    "    docs = [' '.join(w for w in words if w not in stopwords) for words in docs]\n",
    "    return docs\n",
    "\n",
    "\n",
    "def tokenize(text, vocabulary, synonyms=SYNONYMS, stopwords=STOPWORDS):\n",
    "    doc = normalize_corpus_words([text.lower()], synonyms=synonyms, stopwords=stopwords)[0]\n",
    "    stems = [w for w in doc.split() if w in vocabulary]\n",
    "    return stems\n",
    "\n",
    "\n",
    "fun_words = vocabulary = 'cat dog apple lion nyc love big small'\n",
    "fun_stems = normalize_corpus_words([fun_words])[0].split()[:NUM_WORDS]\n",
    "fun_words = fun_words.split()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "17685a44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cat</th>\n",
       "      <th>dog</th>\n",
       "      <th>apple</th>\n",
       "      <th>lion</th>\n",
       "      <th>nyc</th>\n",
       "      <th>love</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.79</td>\n",
       "      <td></td>\n",
       "      <td>0.62</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.79</td>\n",
       "      <td></td>\n",
       "      <td>0.62</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.79</td>\n",
       "      <td></td>\n",
       "      <td>0.62</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.79</td>\n",
       "      <td></td>\n",
       "      <td>0.62</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.0</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.5</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.87</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.54</td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td></td>\n",
       "      <td>0.66</td>\n",
       "      <td>0.75</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    cat dog apple  lion   nyc  love\n",
       "0            0.79        0.62      \n",
       "1            0.79        0.62      \n",
       "2                        0.66  0.75\n",
       "3            0.79        0.62      \n",
       "4            0.79        0.62      \n",
       "5             1.0                  \n",
       "6   1.0                            \n",
       "7   0.5            0.87            \n",
       "8  0.54                        0.84\n",
       "9                        0.66  0.75"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#corpus = get_data('cats_and_dogs_sorted')[:NUM_PRETTY]\n",
    "f = open(\"CatsAndDogs.txt\", \"r\")\n",
    "corpus= f.readlines()\n",
    "\n",
    "docs = normalize_corpus_words(corpus)\n",
    "tfidfer = TfidfVectorizer(min_df=1, max_df=.99, stop_words=None, token_pattern=r'(?u)\\b\\w+\\b',\n",
    "                          vocabulary=fun_stems)\n",
    "tfidf_dense = pd.DataFrame(tfidfer.fit_transform(docs).todense())\n",
    "id_words = [(i, w) for (w, i) in tfidfer.vocabulary_.items()]\n",
    "tfidf_dense.columns = list(zip(*sorted(id_words)))[1]\n",
    "tfidfer.use_idf = False\n",
    "tfidfer.norm = None\n",
    "bow_dense = pd.DataFrame(tfidfer.fit_transform(docs).todense())\n",
    "bow_dense.columns = list(zip(*sorted(id_words)))[1]\n",
    "bow_dense = bow_dense.astype(int)\n",
    "tfidfer.use_idf = True\n",
    "tfidfer.norm = 'l2'\n",
    "bow_pretty = bow_dense.copy()\n",
    "bow_pretty = bow_pretty[fun_stems]\n",
    "bow_pretty['text'] = corpus\n",
    "for col in fun_stems:\n",
    "    bow_pretty.loc[bow_pretty[col] == 0, col] = ''\n",
    "# print(bow_pretty)\n",
    "word_tfidf_dense = pd.DataFrame(tfidfer.transform(fun_stems).todense())\n",
    "word_tfidf_dense.columns = list(zip(*sorted(id_words)))[1]\n",
    "word_tfidf_dense.index = fun_stems\n",
    "\n",
    "tfidf_pretty = tfidf_dense.copy()\n",
    "tfidf_pretty = tfidf_pretty[fun_stems]\n",
    "tfidf_pretty = tfidf_pretty.round(2)\n",
    "for col in fun_stems:\n",
    "    tfidf_pretty.loc[tfidf_pretty[col] == 0, col] = ''\n",
    "\n",
    "\n",
    "tfidf_pretty.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ace3f47",
   "metadata": {},
   "source": [
    "Start semantic analysis using PCA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "f3a13bb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>top0</th>\n",
       "      <th>top1</th>\n",
       "      <th>top2</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>0.7</td>\n",
       "      <td>NYC is the Big Apple.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>0.7</td>\n",
       "      <td>NYC is known as the Big Apple.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>0.6</td>\n",
       "      <td>I love NYC!\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>0.7</td>\n",
       "      <td>I wore a hat to the Big Apple party in NYC.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>0.7</td>\n",
       "      <td>Come to NYC. See the Big Apple!\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>Are you a vet?\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>My flowers are blooming.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>A single flower grew in Benji's grave.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>Char chased the squirrel.\\n</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>I gnawed the frog legs with my teeth.\\n</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>263 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     top0  top1  top2                                           text\n",
       "0    -0.1  -0.3   0.7                        NYC is the Big Apple.\\n\n",
       "1    -0.1  -0.3   0.7               NYC is known as the Big Apple.\\n\n",
       "2    -0.1  -0.3   0.6                                  I love NYC!\\n\n",
       "3    -0.1  -0.3   0.7  I wore a hat to the Big Apple party in NYC.\\n\n",
       "4    -0.1  -0.3   0.7              Come to NYC. See the Big Apple!\\n\n",
       "..    ...   ...   ...                                            ...\n",
       "258  -0.1  -0.2  -0.1                               Are you a vet?\\n\n",
       "259  -0.1  -0.2  -0.1                     My flowers are blooming.\\n\n",
       "260  -0.1  -0.2  -0.1       A single flower grew in Benji's grave.\\n\n",
       "261  -0.1  -0.2  -0.1                    Char chased the squirrel.\\n\n",
       "262  -0.1  -0.2  -0.1        I gnawed the frog legs with my teeth.\\n\n",
       "\n",
       "[263 rows x 4 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_zeros = tfidf_dense.T.sum()[tfidf_dense.T.sum() == 0]\n",
    "pcaer = PCA(n_components=NUM_TOPICS)\n",
    "\n",
    "doc_topic_vectors = pd.DataFrame(pcaer.fit_transform(tfidf_dense.values), columns=['top{}'.format(i) for i in range(NUM_TOPICS)])\n",
    "doc_topic_vectors['text'] = corpus\n",
    "pd.options.display.max_colwidth = 55\n",
    "doc_topic_vectors.round(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757260d9",
   "metadata": {},
   "source": [
    "Now we connect terms with topics and text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4d7496aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_topic_vectors = pd.DataFrame(pcaer.transform(word_tfidf_dense.values), columns=['top{}'.format(i) for i in range(NUM_TOPICS)])\n",
    "word_topic_vectors.index = fun_stems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0a64dd9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>top0</th>\n",
       "      <th>top1</th>\n",
       "      <th>top2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>0.800488</td>\n",
       "      <td>0.296340</td>\n",
       "      <td>0.016234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>-0.555923</td>\n",
       "      <td>0.695405</td>\n",
       "      <td>0.018666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>apple</th>\n",
       "      <td>-0.087553</td>\n",
       "      <td>-0.216128</td>\n",
       "      <td>0.117937</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lion</th>\n",
       "      <td>-0.062965</td>\n",
       "      <td>-0.192958</td>\n",
       "      <td>-0.128977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nyc</th>\n",
       "      <td>-0.111172</td>\n",
       "      <td>-0.298512</td>\n",
       "      <td>0.880894</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>-0.085256</td>\n",
       "      <td>-0.247502</td>\n",
       "      <td>0.012591</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           top0      top1      top2\n",
       "cat    0.800488  0.296340  0.016234\n",
       "dog   -0.555923  0.695405  0.018666\n",
       "apple -0.087553 -0.216128  0.117937\n",
       "lion  -0.062965 -0.192958 -0.128977\n",
       "nyc   -0.111172 -0.298512  0.880894\n",
       "love  -0.085256 -0.247502  0.012591"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_topic_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0213bf75",
   "metadata": {},
   "source": [
    "This is exactly what we want - we associated terms with topics automatically."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87240fea",
   "metadata": {},
   "source": [
    "### Return to SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "799a27d6",
   "metadata": {},
   "source": [
    "We now return to the analysis of SVD properties. It will aslo show how PCA is based on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "2b5ab64a",
   "metadata": {},
   "outputs": [],
   "source": [
    "U, Sigma, VT = np.linalg.svd(tfidf_dense.T)  # <1> Transpose the doc-word tfidf matrix, because SVD works on column vectors\n",
    "S = pd.DataFrame(np.diag(Sigma.copy()))\n",
    "doc_labels = ['doc{}'.format(i) for i in range(len(tfidf_dense))]\n",
    "U_df = pd.DataFrame(U, index=fun_stems)\n",
    "VT_df = pd.DataFrame(VT, index=doc_labels, columns=doc_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe743a6a",
   "metadata": {},
   "source": [
    "We have the following matrices:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796052a3",
   "metadata": {},
   "source": [
    "<img src = \"Singular_value_decomposition_visualisation.svg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfb4d180",
   "metadata": {},
   "source": [
    "#### U - left singular vectors (term-topic matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93b06017",
   "metadata": {},
   "source": [
    "The left singular vectors tell you how to \"rotate\" the TF-IDF vectors into the topic space, equivalent to creating topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92c7d4f8",
   "metadata": {},
   "source": [
    "The U matrix contains the term-topic matrix that tells you about “the company a word keeps.” This is the most important matrix for semantic analysis in NLP. The U matrix is called the “left singular vectors” because it contains row vectors that should be multiplied by a matrix of column vectors from the left. U is the cross-correlation between words and topics based on word co-occurrence in the same document. It’s a square matrix until you start truncating it (deleting columns). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "f4517ece",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>-0.991106</td>\n",
       "      <td>0.128944</td>\n",
       "      <td>0.019021</td>\n",
       "      <td>0.010894</td>\n",
       "      <td>0.024432</td>\n",
       "      <td>-0.002270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>-0.128622</td>\n",
       "      <td>-0.991430</td>\n",
       "      <td>0.022126</td>\n",
       "      <td>-0.004297</td>\n",
       "      <td>-0.000856</td>\n",
       "      <td>-0.003660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>apple</th>\n",
       "      <td>-0.001022</td>\n",
       "      <td>-0.001375</td>\n",
       "      <td>-0.210788</td>\n",
       "      <td>0.091791</td>\n",
       "      <td>-0.001459</td>\n",
       "      <td>-0.973210</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lion</th>\n",
       "      <td>-0.024051</td>\n",
       "      <td>0.004013</td>\n",
       "      <td>0.002123</td>\n",
       "      <td>0.003015</td>\n",
       "      <td>-0.999695</td>\n",
       "      <td>0.001343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>nyc</th>\n",
       "      <td>-0.019346</td>\n",
       "      <td>-0.020460</td>\n",
       "      <td>-0.953934</td>\n",
       "      <td>0.196251</td>\n",
       "      <td>-0.000748</td>\n",
       "      <td>0.225173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>love</th>\n",
       "      <td>-0.014555</td>\n",
       "      <td>0.001572</td>\n",
       "      <td>-0.211479</td>\n",
       "      <td>-0.976173</td>\n",
       "      <td>-0.003099</td>\n",
       "      <td>-0.046249</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              0         1         2         3         4         5\n",
       "cat   -0.991106  0.128944  0.019021  0.010894  0.024432 -0.002270\n",
       "dog   -0.128622 -0.991430  0.022126 -0.004297 -0.000856 -0.003660\n",
       "apple -0.001022 -0.001375 -0.210788  0.091791 -0.001459 -0.973210\n",
       "lion  -0.024051  0.004013  0.002123  0.003015 -0.999695  0.001343\n",
       "nyc   -0.019346 -0.020460 -0.953934  0.196251 -0.000748  0.225173\n",
       "love  -0.014555  0.001572 -0.211479 -0.976173 -0.003099 -0.046249"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d972fd2e",
   "metadata": {},
   "source": [
    "#### S - singular values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56521273",
   "metadata": {},
   "source": [
    "The Sigma or S matrix contains the topic “singular values” in a square diagonal matrix.The singular values tell you how much information is captured by each dimension in your new semantic (topic) vector space. A diagonal matrix has nonzero values only along the diagonal from the upper left to the lower right."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "1fe8afbe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6.342806</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>5.688725</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.502347</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.76255</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>2.115862</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.741329</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2        3         4         5\n",
       "0  6.342806  0.000000  0.000000  0.00000  0.000000  0.000000\n",
       "1  0.000000  5.688725  0.000000  0.00000  0.000000  0.000000\n",
       "2  0.000000  0.000000  3.502347  0.00000  0.000000  0.000000\n",
       "3  0.000000  0.000000  0.000000  2.76255  0.000000  0.000000\n",
       "4  0.000000  0.000000  0.000000  0.00000  2.115862  0.000000\n",
       "5  0.000000  0.000000  0.000000  0.00000  0.000000  1.741329"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bd894b8",
   "metadata": {},
   "source": [
    "#### $V^T$ right singular vectors\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ded64bf",
   "metadata": {},
   "source": [
    "The $V^T$ matrix contains the “right singular vectors” as the columns of the document-document matrix. This gives you the shared meaning between documents, because it measures how often documents use the same topics in your new semantic model of the documents. It has the same number of rows (p) and columns as you have documents in your small corpus.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "571565e3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>doc0</th>\n",
       "      <th>doc1</th>\n",
       "      <th>doc2</th>\n",
       "      <th>doc3</th>\n",
       "      <th>doc4</th>\n",
       "      <th>doc5</th>\n",
       "      <th>doc6</th>\n",
       "      <th>doc7</th>\n",
       "      <th>...</th>\n",
       "      <th>doc255</th>\n",
       "      <th>doc256</th>\n",
       "      <th>doc257</th>\n",
       "      <th>doc258</th>\n",
       "      <th>doc259</th>\n",
       "      <th>doc260</th>\n",
       "      <th>doc261</th>\n",
       "      <th>doc262</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>doc0</th>\n",
       "      <td>-0.002006</td>\n",
       "      <td>-0.002006</td>\n",
       "      <td>-0.003736</td>\n",
       "      <td>-0.002006</td>\n",
       "      <td>-0.002006</td>\n",
       "      <td>-0.000161</td>\n",
       "      <td>-0.156257</td>\n",
       "      <td>-0.081561</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc1</th>\n",
       "      <td>-0.002407</td>\n",
       "      <td>-0.002407</td>\n",
       "      <td>-0.002162</td>\n",
       "      <td>-0.002407</td>\n",
       "      <td>-0.002407</td>\n",
       "      <td>-0.000242</td>\n",
       "      <td>0.022667</td>\n",
       "      <td>0.011966</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc2</th>\n",
       "      <td>-0.215227</td>\n",
       "      <td>-0.215227</td>\n",
       "      <td>-0.224876</td>\n",
       "      <td>-0.215227</td>\n",
       "      <td>-0.215227</td>\n",
       "      <td>-0.060185</td>\n",
       "      <td>0.005431</td>\n",
       "      <td>0.003245</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc3</th>\n",
       "      <td>0.069942</td>\n",
       "      <td>0.069942</td>\n",
       "      <td>-0.219016</td>\n",
       "      <td>0.069942</td>\n",
       "      <td>0.069942</td>\n",
       "      <td>0.033227</td>\n",
       "      <td>0.003944</td>\n",
       "      <td>0.002920</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc4</th>\n",
       "      <td>-0.000761</td>\n",
       "      <td>-0.000761</td>\n",
       "      <td>-0.001335</td>\n",
       "      <td>-0.000761</td>\n",
       "      <td>-0.000761</td>\n",
       "      <td>-0.000690</td>\n",
       "      <td>0.011547</td>\n",
       "      <td>-0.403128</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc5</th>\n",
       "      <td>-0.360514</td>\n",
       "      <td>-0.360514</td>\n",
       "      <td>0.065218</td>\n",
       "      <td>-0.360514</td>\n",
       "      <td>-0.360514</td>\n",
       "      <td>-0.558889</td>\n",
       "      <td>-0.001304</td>\n",
       "      <td>0.000014</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc6</th>\n",
       "      <td>-0.134383</td>\n",
       "      <td>0.027968</td>\n",
       "      <td>0.053156</td>\n",
       "      <td>0.033837</td>\n",
       "      <td>0.021439</td>\n",
       "      <td>0.040278</td>\n",
       "      <td>0.974950</td>\n",
       "      <td>-0.008370</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc7</th>\n",
       "      <td>-0.003905</td>\n",
       "      <td>0.077428</td>\n",
       "      <td>0.165968</td>\n",
       "      <td>-0.328559</td>\n",
       "      <td>0.075929</td>\n",
       "      <td>0.141068</td>\n",
       "      <td>-0.008205</td>\n",
       "      <td>0.830735</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc8</th>\n",
       "      <td>-0.116764</td>\n",
       "      <td>-0.028473</td>\n",
       "      <td>-0.141993</td>\n",
       "      <td>-0.025281</td>\n",
       "      <td>-0.107706</td>\n",
       "      <td>0.219134</td>\n",
       "      <td>-0.012857</td>\n",
       "      <td>-0.004296</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>doc9</th>\n",
       "      <td>-0.078585</td>\n",
       "      <td>-0.078585</td>\n",
       "      <td>-0.251298</td>\n",
       "      <td>-0.078585</td>\n",
       "      <td>0.061376</td>\n",
       "      <td>0.137343</td>\n",
       "      <td>0.001279</td>\n",
       "      <td>0.000427</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 263 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          doc0      doc1      doc2      doc3      doc4      doc5      doc6      doc7  ...  doc255  doc256  doc257  doc258  doc259  doc260  doc261  \\\n",
       "doc0 -0.002006 -0.002006 -0.003736 -0.002006 -0.002006 -0.000161 -0.156257 -0.081561  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "doc1 -0.002407 -0.002407 -0.002162 -0.002407 -0.002407 -0.000242  0.022667  0.011966  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "doc2 -0.215227 -0.215227 -0.224876 -0.215227 -0.215227 -0.060185  0.005431  0.003245  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "doc3  0.069942  0.069942 -0.219016  0.069942  0.069942  0.033227  0.003944  0.002920  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "doc4 -0.000761 -0.000761 -0.001335 -0.000761 -0.000761 -0.000690  0.011547 -0.403128  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "doc5 -0.360514 -0.360514  0.065218 -0.360514 -0.360514 -0.558889 -0.001304  0.000014  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "doc6 -0.134383  0.027968  0.053156  0.033837  0.021439  0.040278  0.974950 -0.008370  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "doc7 -0.003905  0.077428  0.165968 -0.328559  0.075929  0.141068 -0.008205  0.830735  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "doc8 -0.116764 -0.028473 -0.141993 -0.025281 -0.107706  0.219134 -0.012857 -0.004296  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "doc9 -0.078585 -0.078585 -0.251298 -0.078585  0.061376  0.137343  0.001279  0.000427  ...     0.0     0.0     0.0     0.0     0.0     0.0     0.0   \n",
       "\n",
       "      doc262  \n",
       "doc0     0.0  \n",
       "doc1     0.0  \n",
       "doc2     0.0  \n",
       "doc3     0.0  \n",
       "doc4     0.0  \n",
       "doc5     0.0  \n",
       "doc6     0.0  \n",
       "doc7     0.0  \n",
       "doc8     0.0  \n",
       "doc9     0.0  \n",
       "\n",
       "[10 rows x 263 columns]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VT_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "899c21c3",
   "metadata": {},
   "source": [
    "## Truncated SVD"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3eac0d23",
   "metadata": {},
   "source": [
    "## idea:\n",
    "err = []\n",
    "S_tmp=S.copy()\n",
    "for numdim in range(len(S), 0, -1):\n",
    "    S_tmp[numdim - 1, numdim - 1] = 0\n",
    "    reconstructed_tdm = U.dot(S_tmp).dot(VT_df)\n",
    "    err.append(np.sqrt(((reconstructed_tdm - tdm).values.flatten() ** 2).sum() / np.product(tdm.shape)))\n",
    "np.array(err).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02ca41ad",
   "metadata": {},
   "source": [
    "<img src=\"TruncatedSVM.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f336a8b2",
   "metadata": {},
   "source": [
    "Truncated SVD is realized in [scikit learn library](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.TruncatedSVD.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bbe89eb",
   "metadata": {},
   "source": [
    "## Truncated SVD for SMS semantic analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "735f35ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic0</th>\n",
       "      <th>topic1</th>\n",
       "      <th>topic2</th>\n",
       "      <th>topic3</th>\n",
       "      <th>topic4</th>\n",
       "      <th>topic5</th>\n",
       "      <th>topic6</th>\n",
       "      <th>topic7</th>\n",
       "      <th>topic8</th>\n",
       "      <th>topic9</th>\n",
       "      <th>topic10</th>\n",
       "      <th>topic11</th>\n",
       "      <th>topic12</th>\n",
       "      <th>topic13</th>\n",
       "      <th>topic14</th>\n",
       "      <th>topic15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sms0</th>\n",
       "      <td>0.201</td>\n",
       "      <td>0.003</td>\n",
       "      <td>0.037</td>\n",
       "      <td>0.011</td>\n",
       "      <td>-0.019</td>\n",
       "      <td>-0.053</td>\n",
       "      <td>0.039</td>\n",
       "      <td>-0.066</td>\n",
       "      <td>0.012</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>0.007</td>\n",
       "      <td>-0.007</td>\n",
       "      <td>0.002</td>\n",
       "      <td>-0.036</td>\n",
       "      <td>-0.014</td>\n",
       "      <td>0.037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms1</th>\n",
       "      <td>0.404</td>\n",
       "      <td>-0.094</td>\n",
       "      <td>-0.078</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.100</td>\n",
       "      <td>0.047</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.065</td>\n",
       "      <td>0.023</td>\n",
       "      <td>-0.024</td>\n",
       "      <td>-0.004</td>\n",
       "      <td>0.036</td>\n",
       "      <td>0.043</td>\n",
       "      <td>-0.021</td>\n",
       "      <td>0.051</td>\n",
       "      <td>-0.042</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms2!</th>\n",
       "      <td>-0.030</td>\n",
       "      <td>-0.048</td>\n",
       "      <td>0.090</td>\n",
       "      <td>-0.067</td>\n",
       "      <td>0.091</td>\n",
       "      <td>-0.043</td>\n",
       "      <td>-0.000</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>-0.057</td>\n",
       "      <td>0.051</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.026</td>\n",
       "      <td>-0.020</td>\n",
       "      <td>-0.042</td>\n",
       "      <td>0.052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms3</th>\n",
       "      <td>0.329</td>\n",
       "      <td>-0.033</td>\n",
       "      <td>-0.035</td>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.052</td>\n",
       "      <td>0.056</td>\n",
       "      <td>-0.166</td>\n",
       "      <td>-0.074</td>\n",
       "      <td>0.063</td>\n",
       "      <td>-0.108</td>\n",
       "      <td>0.022</td>\n",
       "      <td>0.023</td>\n",
       "      <td>0.073</td>\n",
       "      <td>-0.046</td>\n",
       "      <td>0.022</td>\n",
       "      <td>-0.070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms4</th>\n",
       "      <td>0.002</td>\n",
       "      <td>0.031</td>\n",
       "      <td>0.038</td>\n",
       "      <td>0.034</td>\n",
       "      <td>-0.075</td>\n",
       "      <td>-0.093</td>\n",
       "      <td>-0.044</td>\n",
       "      <td>0.061</td>\n",
       "      <td>-0.045</td>\n",
       "      <td>0.029</td>\n",
       "      <td>0.028</td>\n",
       "      <td>-0.009</td>\n",
       "      <td>0.027</td>\n",
       "      <td>0.034</td>\n",
       "      <td>-0.083</td>\n",
       "      <td>-0.021</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms5!</th>\n",
       "      <td>-0.016</td>\n",
       "      <td>0.059</td>\n",
       "      <td>0.014</td>\n",
       "      <td>-0.006</td>\n",
       "      <td>0.122</td>\n",
       "      <td>-0.040</td>\n",
       "      <td>0.005</td>\n",
       "      <td>0.167</td>\n",
       "      <td>-0.023</td>\n",
       "      <td>0.064</td>\n",
       "      <td>0.041</td>\n",
       "      <td>0.055</td>\n",
       "      <td>-0.037</td>\n",
       "      <td>0.075</td>\n",
       "      <td>-0.001</td>\n",
       "      <td>0.020</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       topic0  topic1  topic2  topic3  topic4  topic5  topic6  topic7  topic8  topic9  topic10  topic11  topic12  topic13  topic14  topic15\n",
       "sms0    0.201   0.003   0.037   0.011  -0.019  -0.053   0.039  -0.066   0.012  -0.083    0.007   -0.007    0.002   -0.036   -0.014    0.037\n",
       "sms1    0.404  -0.094  -0.078   0.051   0.100   0.047   0.023   0.065   0.023  -0.024   -0.004    0.036    0.043   -0.021    0.051   -0.042\n",
       "sms2!  -0.030  -0.048   0.090  -0.067   0.091  -0.043  -0.000  -0.001  -0.057   0.051    0.125    0.023    0.026   -0.020   -0.042    0.052\n",
       "sms3    0.329  -0.033  -0.035  -0.016   0.052   0.056  -0.166  -0.074   0.063  -0.108    0.022    0.023    0.073   -0.046    0.022   -0.070\n",
       "sms4    0.002   0.031   0.038   0.034  -0.075  -0.093  -0.044   0.061  -0.045   0.029    0.028   -0.009    0.027    0.034   -0.083   -0.021\n",
       "sms5!  -0.016   0.059   0.014  -0.006   0.122  -0.040   0.005   0.167  -0.023   0.064    0.041    0.055   -0.037    0.075   -0.001    0.020"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd = TruncatedSVD(n_components=16, n_iter=100)\n",
    "svd_topic_vectors = svd.fit_transform(tfidf_docs.values)\n",
    "svd_topic_vectors = pd.DataFrame(svd_topic_vectors, columns=columns, index=index)\n",
    "svd_topic_vectors.round(3).head(6)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31c1456c",
   "metadata": {},
   "source": [
    "We can now check how good is TruncatedSVD in spam analysis. Compute cos-similarity beetween messages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b2514fe6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sms0</th>\n",
       "      <th>sms1</th>\n",
       "      <th>sms2!</th>\n",
       "      <th>sms3</th>\n",
       "      <th>sms4</th>\n",
       "      <th>sms5!</th>\n",
       "      <th>sms6</th>\n",
       "      <th>sms7</th>\n",
       "      <th>sms8!</th>\n",
       "      <th>sms9!</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sms0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.6</td>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms1</th>\n",
       "      <td>0.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms2!</th>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.5</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms3</th>\n",
       "      <td>0.6</td>\n",
       "      <td>0.8</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms4</th>\n",
       "      <td>-0.0</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>-0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms5!</th>\n",
       "      <td>-0.3</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.4</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.3</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms6</th>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms7</th>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.3</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.1</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms8!</th>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.5</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>-0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms9!</th>\n",
       "      <td>-0.3</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>0.4</td>\n",
       "      <td>-0.1</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>-0.2</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.3</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sms0  sms1  sms2!  sms3  sms4  sms5!  sms6  sms7  sms8!  sms9!\n",
       "sms0    1.0   0.6   -0.1   0.6  -0.0   -0.3  -0.3  -0.1   -0.3   -0.3\n",
       "sms1    0.6   1.0   -0.2   0.8  -0.2    0.0  -0.2  -0.2   -0.1   -0.1\n",
       "sms2!  -0.1  -0.2    1.0  -0.2   0.1    0.4   0.0   0.3    0.5    0.4\n",
       "sms3    0.6   0.8   -0.2   1.0  -0.2   -0.3  -0.1  -0.3   -0.2   -0.1\n",
       "sms4   -0.0  -0.2    0.1  -0.2   1.0    0.2   0.0   0.1   -0.4   -0.2\n",
       "sms5!  -0.3   0.0    0.4  -0.3   0.2    1.0  -0.1   0.1    0.3    0.4\n",
       "sms6   -0.3  -0.2    0.0  -0.1   0.0   -0.1   1.0   0.1   -0.2   -0.2\n",
       "sms7   -0.1  -0.2    0.3  -0.3   0.1    0.1   0.1   1.0    0.1    0.4\n",
       "sms8!  -0.3  -0.1    0.5  -0.2  -0.4    0.3  -0.2   0.1    1.0    0.3\n",
       "sms9!  -0.3  -0.1    0.4  -0.1  -0.2    0.4  -0.2   0.4    0.3    1.0"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "svd_topic_vectors = (svd_topic_vectors.T / np.linalg.norm(\\\n",
    "svd_topic_vectors, axis=1)).T\n",
    "svd_topic_vectors.iloc[:10].dot(svd_topic_vectors.iloc[:10].T).round(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5378d683",
   "metadata": {},
   "source": [
    "You should see larger positive cosine similarity (dot products) between any spam message (“sms2!”)!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f1bdeb",
   "metadata": {},
   "source": [
    "This gives you and idea for spam detection."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5193bc29",
   "metadata": {},
   "source": [
    "##  Latent Dirichlet allocation (LDiA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ebbbb0",
   "metadata": {},
   "source": [
    "[LDiA](https://en.wikipedia.org/wiki/Latent_Dirichlet_allocation) creates a semantic vector space model (like your topic vectors) using an\n",
    "approach similar to how your brain worked - you manually allocated words to topics based on how often they occurred together in the same document. The topic mix for a document can then be determined by the word mixtures in each topic by which topic those words were assigned to. This makes an LDiA topic model much easier to\n",
    "understand, because the words assigned to topics and topics assigned to documents tend to make more sense than for LSA.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e601115",
   "metadata": {},
   "source": [
    "The LDiA approach was developed in 2000 by geneticists in the UK to help them “infer population structure” from sequences of genes. Stanford Researchers (including Andrew Ng) popularized the approach for NLP in 2003."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76986b68",
   "metadata": {},
   "source": [
    "LDiA is realized in [scikit learn library](https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.LatentDirichletAllocation.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f3a0cb",
   "metadata": {},
   "source": [
    "#### LDiA example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb6340d",
   "metadata": {},
   "source": [
    "Create BOW vectors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "82666e40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>!</th>\n",
       "      <th>\"</th>\n",
       "      <th>#</th>\n",
       "      <th>#150</th>\n",
       "      <th>#5000</th>\n",
       "      <th>$</th>\n",
       "      <th>%</th>\n",
       "      <th>&amp;</th>\n",
       "      <th>...</th>\n",
       "      <th>—</th>\n",
       "      <th>‘</th>\n",
       "      <th>’</th>\n",
       "      <th>“</th>\n",
       "      <th>…</th>\n",
       "      <th>┾</th>\n",
       "      <th>〨ud</th>\n",
       "      <th>鈥</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sms0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms2!</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms4</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms4832!</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms4833</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms4834</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms4835</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms4836</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4837 rows × 9232 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          !  \"  #  #150  #5000  $  %  &  ...  —  ‘  ’  “  …  ┾  〨ud  鈥\n",
       "sms0      0  0  0     0      0  0  0  0  ...  0  0  0  0  0  0    0  0\n",
       "sms1      0  0  0     0      0  0  0  0  ...  0  0  0  0  0  0    0  0\n",
       "sms2!     0  0  0     0      0  0  0  1  ...  0  0  0  0  0  0    0  0\n",
       "sms3      0  0  0     0      0  0  0  0  ...  0  0  0  0  0  0    0  0\n",
       "sms4      0  0  0     0      0  0  0  0  ...  0  0  0  0  0  0    0  0\n",
       "...      .. .. ..   ...    ... .. .. ..  ... .. .. .. .. .. ..  ... ..\n",
       "sms4832!  1  0  0     0      0  0  0  0  ...  0  0  0  0  0  0    0  0\n",
       "sms4833   0  0  0     0      0  0  0  0  ...  0  0  0  0  0  0    0  0\n",
       "sms4834   0  0  0     0      0  0  0  0  ...  0  0  0  0  0  0    0  0\n",
       "sms4835   0  0  0     0      0  0  0  0  ...  0  0  0  0  0  0    0  0\n",
       "sms4836   0  0  0     0      0  0  0  0  ...  0  0  0  0  0  0    0  0\n",
       "\n",
       "[4837 rows x 9232 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.tokenize import casual_tokenize\n",
    "np.random.seed(42)\n",
    "counter = CountVectorizer(tokenizer=casual_tokenize)\n",
    "bow_docs = pd.DataFrame(counter.fit_transform(raw_documents=sms.text).toarray(), index=index)\n",
    "column_nums, terms = zip(*sorted(zip(counter.vocabulary_.values(), counter.vocabulary_.keys())))\n",
    "bow_docs.columns = terms\n",
    "bow_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "ee3049fb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms.loc['sms0'].text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "634b4a6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       ",            1\n",
       "..           1\n",
       "...          2\n",
       "amore        1\n",
       "available    1\n",
       "Name: sms0, dtype: int64"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bow_docs.loc['sms0'][bow_docs.loc['sms0'] > 0].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e011cb72",
   "metadata": {},
   "source": [
    "Make LDiA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "92c087b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(16, 9232)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.decomposition import LatentDirichletAllocation as LDiA\n",
    "ldia = LDiA(n_components=16, learning_method='batch')\n",
    "ldia = ldia.fit(bow_docs)\n",
    "ldia.components_.shape\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "21401a30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic0</th>\n",
       "      <th>topic1</th>\n",
       "      <th>topic2</th>\n",
       "      <th>topic3</th>\n",
       "      <th>topic4</th>\n",
       "      <th>topic5</th>\n",
       "      <th>topic6</th>\n",
       "      <th>topic7</th>\n",
       "      <th>topic8</th>\n",
       "      <th>topic9</th>\n",
       "      <th>topic10</th>\n",
       "      <th>topic11</th>\n",
       "      <th>topic12</th>\n",
       "      <th>topic13</th>\n",
       "      <th>topic14</th>\n",
       "      <th>topic15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>!</th>\n",
       "      <td>184.03</td>\n",
       "      <td>15.00</td>\n",
       "      <td>72.22</td>\n",
       "      <td>394.95</td>\n",
       "      <td>45.48</td>\n",
       "      <td>36.14</td>\n",
       "      <td>9.55</td>\n",
       "      <td>44.81</td>\n",
       "      <td>0.43</td>\n",
       "      <td>90.23</td>\n",
       "      <td>37.42</td>\n",
       "      <td>44.18</td>\n",
       "      <td>64.40</td>\n",
       "      <td>297.29</td>\n",
       "      <td>41.16</td>\n",
       "      <td>11.70</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>\"</th>\n",
       "      <td>0.68</td>\n",
       "      <td>4.22</td>\n",
       "      <td>2.41</td>\n",
       "      <td>0.06</td>\n",
       "      <td>152.35</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.68</td>\n",
       "      <td>8.42</td>\n",
       "      <td>11.42</td>\n",
       "      <td>0.07</td>\n",
       "      <td>62.72</td>\n",
       "      <td>12.27</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>#</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>2.07</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "      <td>1.07</td>\n",
       "      <td>4.05</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.06</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   topic0  topic1  topic2  topic3  topic4  topic5  topic6  topic7  \\\n",
       "!  184.03   15.00   72.22  394.95   45.48   36.14    9.55   44.81   \n",
       "\"    0.68    4.22    2.41    0.06  152.35    0.06    0.06    0.06   \n",
       "#    0.06    0.06    0.06    0.06    0.06    2.07    0.06    0.06   \n",
       "\n",
       "   topic8  topic9  topic10  topic11  topic12  topic13  topic14  topic15  \n",
       "!    0.43   90.23    37.42    44.18    64.40   297.29    41.16    11.70  \n",
       "\"    0.45    0.68     8.42    11.42     0.07    62.72    12.27     0.06  \n",
       "#    0.06    0.06     0.06     0.06     1.07     4.05     0.06     0.06  "
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.set_option('display.width', 75)\n",
    "components = pd.DataFrame(ldia.components_.T, index=terms, columns=columns)\n",
    "components.round(2).head(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "35b1b0ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "!       394.952246\n",
       ".       218.049724\n",
       "to      119.533134\n",
       "u       118.857546\n",
       "call    111.948541\n",
       "£       107.358914\n",
       ",        96.954384\n",
       "*        90.314783\n",
       "your     90.215961\n",
       "is       75.750037\n",
       "Name: topic3, dtype: float64"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "components.topic3.sort_values(ascending=False)[:10]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4812ab",
   "metadata": {},
   "source": [
    "Before you fit your LDA classifier, you need to compute these LDiA topic vectors for all your documents (SMS messages). And let’s see how they are different from the topic vectors produced by SVD and PCA for those same documents:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9eb90c41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic0</th>\n",
       "      <th>topic1</th>\n",
       "      <th>topic2</th>\n",
       "      <th>topic3</th>\n",
       "      <th>topic4</th>\n",
       "      <th>topic5</th>\n",
       "      <th>topic6</th>\n",
       "      <th>topic7</th>\n",
       "      <th>topic8</th>\n",
       "      <th>topic9</th>\n",
       "      <th>topic10</th>\n",
       "      <th>topic11</th>\n",
       "      <th>topic12</th>\n",
       "      <th>topic13</th>\n",
       "      <th>topic14</th>\n",
       "      <th>topic15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sms0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.62</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.34</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms1</th>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.78</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms2!</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.98</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.85</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms4</th>\n",
       "      <td>0.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms5!</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.17</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.26</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms6</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms7</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.97</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms8!</th>\n",
       "      <td>0.57</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.25</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms9!</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.43</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       topic0  topic1  topic2  topic3  topic4  topic5  topic6  topic7  \\\n",
       "sms0     0.00    0.62    0.00    0.00    0.00    0.00    0.00    0.00   \n",
       "sms1     0.01    0.01    0.01    0.01    0.01    0.01    0.01    0.01   \n",
       "sms2!    0.00    0.00    0.00    0.00    0.00    0.00    0.00    0.00   \n",
       "sms3     0.00    0.00    0.00    0.00    0.09    0.00    0.00    0.00   \n",
       "sms4     0.39    0.00    0.33    0.00    0.00    0.00    0.14    0.00   \n",
       "sms5!    0.00    0.00    0.28    0.00    0.00    0.00    0.00    0.17   \n",
       "sms6     0.00    0.00    0.50    0.00    0.00    0.00    0.00    0.00   \n",
       "sms7     0.00    0.00    0.00    0.00    0.97    0.00    0.00    0.00   \n",
       "sms8!    0.57    0.00    0.00    0.16    0.00    0.00    0.00    0.00   \n",
       "sms9!    0.00    0.00    0.00    0.43    0.00    0.00    0.00    0.00   \n",
       "\n",
       "       topic8  topic9  topic10  topic11  topic12  topic13  topic14  \\\n",
       "sms0     0.34    0.00     0.00     0.00     0.00     0.00     0.00   \n",
       "sms1     0.78    0.01     0.01     0.12     0.01     0.01     0.01   \n",
       "sms2!    0.00    0.98     0.00     0.00     0.00     0.00     0.00   \n",
       "sms3     0.85    0.00     0.00     0.00     0.00     0.00     0.00   \n",
       "sms4     0.00    0.00     0.00     0.00     0.09     0.00     0.00   \n",
       "sms5!    0.00    0.26     0.05     0.00     0.11     0.08     0.05   \n",
       "sms6     0.00    0.00     0.00     0.00     0.00     0.45     0.00   \n",
       "sms7     0.00    0.00     0.00     0.00     0.00     0.00     0.00   \n",
       "sms8!    0.00    0.00     0.00     0.00     0.00     0.00     0.25   \n",
       "sms9!    0.00    0.11     0.00     0.00     0.00     0.44     0.00   \n",
       "\n",
       "       topic15  \n",
       "sms0      0.00  \n",
       "sms1      0.01  \n",
       "sms2!     0.00  \n",
       "sms3      0.00  \n",
       "sms4      0.00  \n",
       "sms5!     0.00  \n",
       "sms6      0.00  \n",
       "sms7      0.00  \n",
       "sms8!     0.00  \n",
       "sms9!     0.00  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldia16_topic_vectors = ldia.transform(bow_docs)\n",
    "ldia16_topic_vectors = pd.DataFrame(ldia16_topic_vectors, index=index, columns=columns)\n",
    "ldia16_topic_vectors.round(2).head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3bcdfec",
   "metadata": {},
   "source": [
    "You can see that these topics are more cleanly separated. There are a lot of zeros in your allocation of topics to messages. This is one of the things that makes LDiA topics easier to explain to coworkers when making business decisions based on your NLP pipeline results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13febb4f",
   "metadata": {},
   "source": [
    "#### LDiA + LDA = spam classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f90fb9",
   "metadata": {},
   "source": [
    "We use LDiA vector to use in LDA classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "4978bfb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.94"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "X_train, X_test, y_train, y_test =  train_test_split(ldia16_topic_vectors, sms.spam, test_size=0.5, random_state=271828)\n",
    "lda = LDA(n_components=1)\n",
    "lda = lda.fit(X_train, y_train)\n",
    "sms['ldia16_spam'] = lda.predict(ldia16_topic_vectors)\n",
    "round(float(lda.score(X_test, y_test)), 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6751a0b2",
   "metadata": {},
   "source": [
    "94% accuracy on the test set  is pretty good, but not quite as PCA - see below\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "598fcf0e",
   "metadata": {},
   "source": [
    "We compare it with LDA on TF-IDF vectors, i.e., without LDiA:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "85d1f631",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score on train data =  1.0\n",
      "score for test data =  0.748\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize.casual import casual_tokenize\n",
    "tfidf = TfidfVectorizer(tokenizer=casual_tokenize)\n",
    "tfidf_docs = tfidf.fit_transform(raw_documents=sms.text).toarray()\n",
    "tfidf_docs = tfidf_docs - tfidf_docs.mean(axis=0)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(tfidf_docs,sms.spam.values, test_size=0.5, random_state=271828)\n",
    "lda = LDA(n_components=1)\n",
    "lda = lda.fit(X_train, y_train)\n",
    "print(\"score on train data = \", round(float(lda.score(X_train, y_train)), 3))\n",
    "print(\"score for test data = \", round(float(lda.score(X_test, y_test)), 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b879a9b",
   "metadata": {},
   "source": [
    "#### 32 LDiA topics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3010ee85",
   "metadata": {},
   "source": [
    "Above we used 16 topics. Now we increase number of topics to 32:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a3342ca8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32, 9232)"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldia32 = LDiA(n_components=32, learning_method='batch')\n",
    "ldia32 = ldia32.fit(bow_docs)\n",
    "ldia32.components_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "28512b67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>topic0</th>\n",
       "      <th>topic1</th>\n",
       "      <th>topic2</th>\n",
       "      <th>topic3</th>\n",
       "      <th>topic4</th>\n",
       "      <th>topic5</th>\n",
       "      <th>topic6</th>\n",
       "      <th>topic7</th>\n",
       "      <th>...</th>\n",
       "      <th>topic24</th>\n",
       "      <th>topic25</th>\n",
       "      <th>topic26</th>\n",
       "      <th>topic27</th>\n",
       "      <th>topic28</th>\n",
       "      <th>topic29</th>\n",
       "      <th>topic30</th>\n",
       "      <th>topic31</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>sms0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.06</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.53</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms2!</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.65</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.33</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.11</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.39</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sms4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.47</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       topic0  topic1  topic2  topic3  topic4  topic5  topic6  topic7  \\\n",
       "sms0      0.0    0.00     0.0    0.06    0.14    0.00     0.0     0.0   \n",
       "sms1      0.0    0.00     0.0    0.00    0.53    0.00     0.0     0.0   \n",
       "sms2!     0.0    0.00     0.0    0.00    0.00    0.65     0.0     0.0   \n",
       "sms3      0.0    0.11     0.0    0.00    0.39    0.00     0.0     0.0   \n",
       "sms4      0.0    0.00     0.0    0.00    0.00    0.00     0.0     0.0   \n",
       "\n",
       "       ...  topic24  topic25  topic26  topic27  topic28  topic29  \\\n",
       "sms0   ...     0.00     0.00      0.0     0.00      0.0     0.00   \n",
       "sms1   ...     0.00     0.00      0.0     0.00      0.0     0.14   \n",
       "sms2!  ...     0.00     0.33      0.0     0.00      0.0     0.00   \n",
       "sms3   ...     0.00     0.00      0.0     0.00      0.0     0.00   \n",
       "sms4   ...     0.09     0.00      0.0     0.47      0.0     0.00   \n",
       "\n",
       "       topic30  topic31  \n",
       "sms0       0.0      0.0  \n",
       "sms1       0.0      0.0  \n",
       "sms2!      0.0      0.0  \n",
       "sms3       0.0      0.0  \n",
       "sms4       0.0      0.0  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ldia32_topic_vectors = ldia32.transform(bow_docs)\n",
    "columns32 = ['topic{}'.format(i) for i in range(ldia32.n_components)]\n",
    "ldia32_topic_vectors = pd.DataFrame(ldia32_topic_vectors, index=index, columns=columns32)\n",
    "ldia32_topic_vectors.round(2).head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "ecfbb0a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score on train data =  0.933\n",
      "score on test data =  0.936\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(ldia32_topic_vectors, sms.spam, test_size=0.5, random_state=271828)\n",
    "lda = LDA(n_components=1)\n",
    "lda = lda.fit(X_train, y_train)\n",
    "sms['ldia32_spam'] = lda.predict(ldia32_topic_vectors)\n",
    "\n",
    "print(\"score on train data = \", round(float(lda.score(X_train, y_train)), 3))\n",
    "print(\"score on test data = \",  round(float(lda.score(X_test, y_test)), 3))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7358950b",
   "metadata": {},
   "source": [
    "## LDA Spam classifier again"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc2fe61b",
   "metadata": {},
   "source": [
    "We do LDA for TF-IDF vector again using cross-validation for better score estimation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "f7ce5e53",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Accuracy: 0.77 (+/-0.02)'"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "lda = LDA(n_components=1)\n",
    "scores = cross_val_score(lda, tfidf_docs, sms.spam, cv=5)\n",
    "\"Accuracy: {:.2f} (+/-{:.2f})\".format(scores.mean(), scores.std() * 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b280aa4",
   "metadata": {},
   "source": [
    "#### LSA + LDA SMS Spam calssification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca25833b",
   "metadata": {},
   "source": [
    "We use PCA data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "2f79d089",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score on test data =  0.963\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Accuracy: 0.957 (+/-0.022)'"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(pca_topic_vectors.values, sms.spam, test_size=0.3,random_state=271828)\n",
    "lda = LDA(n_components=1)\n",
    "lda.fit(X_train, y_train)\n",
    "lda.fit(X_train, y_train)\n",
    "print(\"score on test data = \", lda.score(X_test, y_test).round(3))\n",
    "\n",
    "lda = LDA(n_components=1)\n",
    "scores = cross_val_score(lda, pca_topic_vectors, sms.spam, cv=10)\n",
    "\"Accuracy: {:.3f} (+/-{:.3f})\".format(scores.mean(), scores.std() * 2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec03bc59",
   "metadata": {},
   "source": [
    "This is higher accuracy than for Latent Dirichlet Analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0923803f",
   "metadata": {},
   "source": [
    "## Topic vs Semantic Search"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3c22040",
   "metadata": {},
   "source": [
    "When you search for a document based on a word or partial word it contains, that’s\n",
    "called **full text search**. This is what search engines do. They break a document into\n",
    "chunks (usually words) that can be indexed with an inverted index like you’d find at the\n",
    "back of a textbook. It takes a lot of bookkeeping and guesswork to deal with spelling\n",
    "errors and typos, but it works pretty well.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b261e15e",
   "metadata": {},
   "source": [
    "[**Semantic search**](https://en.wikipedia.org/wiki/Semantic_search) is full text search that takes into account the meaning of the words\n",
    "in your query and the documents you’re searching. You’ve learned two\n",
    "ways\n",
    "- LSA \n",
    "- LDiA\n",
    "\n",
    "to compute topic vectors that capture the semantics (meaning) of words and documents in a vector. One of the reasons that latent semantic analysis was first called latent semantic indexing was because it promised to power semantic\n",
    "search with an index of numerical values, like BOW and TF-IDF tables. \n",
    "\n",
    "Semantic search was the next big thing in information retrieval."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d05f5e76",
   "metadata": {},
   "source": [
    "Unlike BOW and TF-IDF tables, tables of semantic vectors can’t be easily discretized and indexed using traditional inverted index techniques. Traditional indexing approaches work with binary word occurrence vectors, discrete vectors (BOW vectors), sparse continuous vectors (TF-IDF vectors), and low-dimensional continuous\n",
    "vectors (3D GIS data). \n",
    "\n",
    "But high-dimensional continuous vectors, such as topic vectors from LSA or LDiA, are a challenge.58 Inverted indexes work for discrete vectors or binary vectors, like tables of binary or integer word-document vectors, because the\n",
    "index only needs to maintain an entry for each nonzero discrete dimension. Either that value of that dimension is present or not present in the referenced vector or document. Because TF-IDF vectors are sparse, mostly zero, you don’t need an entry in your index for most dimensions for most documents."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8bd1df3",
   "metadata": {},
   "source": [
    "<img src=\"SemanticSearch.png\">\n",
    "\n",
    "One can note that semantic search get worse around 12 dimensions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68abbc51",
   "metadata": {},
   "source": [
    "## Disntances and similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b05b324",
   "metadata": {},
   "source": [
    "**Distance** measures disnatnce between two vectors. It can be ordinary euclidean distance or any other 'distance', that measures dissimilarity between vectors.\n",
    "\n",
    "Distance is assoicated with mathematical meaning of [metric](https://en.wikipedia.org/wiki/Metric_(mathematics))."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30be599a",
   "metadata": {},
   "source": [
    "Metric must have the following three properties:\n",
    "- Nonnegativity: metrics can never be negative. metric(A, B) >= 0\n",
    "- Indiscerniblity: two objects are identical if the metric between them is zero. if metric(A, B) == 0: assert(A == B)\n",
    "- Symmetry: metrics don’t care about direction. metric(A, B) = metric(B, A)\n",
    "- Triangle inequality: you can’t get from A to C faster by going through B in-between. metric(A, C) <= metric(A, B) + metric(B, C)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49c6af1e",
   "metadata": {},
   "source": [
    "**Similarity** is associated with distance - it ranges between 0.0 (totaly dissimilar) and 1.0 (totally similar).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c01467",
   "metadata": {},
   "source": [
    "Typical conversion formula:\n",
    "- similarity = 1. / (1. + distance)\n",
    "- distance = (1. / similarity) - 1.\n",
    "\n",
    "For distances and similarities ranged in $[0;1]$ interval the conversion formulas are simpler:\n",
    "- similarity = 1. - distance\n",
    "- distance = 1. - similarity\n",
    "\n",
    "Example:"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5ee54b8f",
   "metadata": {},
   "source": [
    "import math\n",
    "angular_distance = math.acos(cosine_similarity) / math.pi\n",
    "distance = 1. / similarity - 1.\n",
    "similarity = 1. - distance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f5b5b21",
   "metadata": {},
   "source": [
    "Standard similarity distances are defined in [scikit learn library](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise_distances.html):\n",
    "\n",
    "- 'cityblock'\n",
    "- 'cosine', \n",
    "- 'euclidean', \n",
    "- 'l1',\n",
    "- 'l2', \n",
    "- 'manhattan', \n",
    "- 'braycurtis',\n",
    "- 'canberra', \n",
    "- 'chebyshev', \n",
    "- 'correlation', \n",
    "- 'dice', \n",
    "- 'hamming', \n",
    "- 'jaccard',\n",
    "- 'kulsinski', \n",
    "- 'mahalanobis', \n",
    "- 'matching', \n",
    "- 'minkowski', \n",
    "- 'rogerstanimoto',\n",
    "- 'russellrao', \n",
    "- 'seuclidean', \n",
    "- 'sokalmichener', \n",
    "- 'sokalsneath', \n",
    "- 'sqeuclidean',\n",
    "- 'yule'\n",
    "\n",
    "Each metric has specific application and there is no-one-for-all solution!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "767eb028",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "0abe801034007484f152c408f6878125c4d199d6c578a45ceffdae6ced931ee7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
