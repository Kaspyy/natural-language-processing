{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1 =     Unnamed: 0  spam                                               text\n",
      "0           0     0  Go until jurong point, crazy.. Available only ...\n",
      "1           1     0                      Ok lar... Joking wif u oni...\n",
      "2           2     1  Free entry in 2 a wkly comp to win FA Cup fina...\n",
      "3           3     0  U dun say so early hor... U c already then say...\n",
      "4           4     0  Nah I don't think he goes to usf, he lives aro...\n",
      "step 2 =  [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "step 3 =         topic0  topic1  topic2  topic3  topic4  topic5  topic6  topic7  topic8  topic9  topic10  topic11  topic12  \\\n",
      "sms0    0.201   0.003   0.037   0.011  -0.019  -0.053   0.039  -0.066   0.013  -0.083    0.010   -0.002    0.001   \n",
      "sms1    0.404  -0.094  -0.078   0.051   0.100   0.047   0.023   0.066   0.021  -0.023   -0.005    0.035    0.040   \n",
      "sms2!  -0.030  -0.048   0.090  -0.067   0.091  -0.043  -0.000  -0.002  -0.057   0.051    0.126    0.022    0.025   \n",
      "sms3    0.329  -0.033  -0.035  -0.016   0.052   0.056  -0.166  -0.073   0.062  -0.108    0.022    0.023    0.068   \n",
      "sms4    0.002   0.031   0.038   0.034  -0.075  -0.092  -0.043   0.062  -0.046   0.029    0.027   -0.008    0.030   \n",
      "sms5!  -0.016   0.059   0.014  -0.006   0.122  -0.040   0.005   0.166  -0.025   0.065    0.041    0.052   -0.032   \n",
      "\n",
      "       topic13  topic14  topic15  \n",
      "sms0    -0.034   -0.016    0.034  \n",
      "sms1    -0.023    0.050   -0.041  \n",
      "sms2!   -0.026   -0.043    0.053  \n",
      "sms3    -0.053    0.024   -0.076  \n",
      "sms4     0.032   -0.082   -0.017  \n",
      "sms5!    0.087   -0.002    0.012  \n",
      "step 4 =  Accuracy: 0.957 (+/-0.023)\n"
     ]
    }
   ],
   "source": [
    "# create spam filter using following setup:\n",
    "# 'sms-spam.csv' contains spam content labeled by 0 (no spam) and 1 (spam)\n",
    "# do the following steps:\n",
    "# 1. read the csv file\n",
    "# 2. make tf-idf vectors\n",
    "# 3. make lsa using pca\n",
    "# 4. use lda classifier for spam classification - make cross validation\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.display.width = 120\n",
    "sms = pd.read_csv('sms-spam.csv')\n",
    "index = ['sms{}{}'.format(i,'!'*j) for (i,j) in zip(range(len(sms)),sms.spam)]\n",
    "print(\"step 1 = \", sms.head(5))\n",
    "\n",
    "# make tf-idf vectors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize.casual import casual_tokenize\n",
    "tfidf = TfidfVectorizer(tokenizer=casual_tokenize)\n",
    "tfidf_docs = tfidf.fit_transform(raw_documents=sms.text).toarray()\n",
    "print(\"step 2 = \", tfidf_docs)\n",
    "\n",
    "# do lsa using pca\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=16)\n",
    "pca = pca.fit(tfidf_docs)\n",
    "pca_topic_vectors = pca.transform(tfidf_docs)\n",
    "columns = ['topic{}'.format(i) for i in range(pca.n_components)]\n",
    "pca_topic_vectors = pd.DataFrame(pca_topic_vectors, index=index, columns=columns)\n",
    "print(\"step 3 = \", pca_topic_vectors.round(3).head(6))\n",
    "\n",
    "# do lda with validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "lda = LDA(n_components=1)\n",
    "scores = cross_val_score(lda, pca_topic_vectors, sms.spam, cv=10)\n",
    "print(\"step 4 = \", \"Accuracy: {:.3f} (+/-{:.3f})\".format(scores.mean(), scores.std()*2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2 =  [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "step 3 =         topic0  topic1  topic2  topic3  topic4  topic5  topic6  topic7  topic8  topic9  topic10  topic11  topic12  \\\n",
      "sms0    0.354   0.011   0.011   0.011   0.286   0.011   0.011   0.011   0.011   0.011    0.011    0.011    0.011   \n",
      "sms1    0.347   0.018   0.018   0.018   0.018   0.018   0.018   0.406   0.018   0.018    0.018    0.018    0.018   \n",
      "sms2!   0.010   0.010   0.503   0.010   0.010   0.010   0.362   0.010   0.010   0.010    0.010    0.010    0.010   \n",
      "sms3    0.572   0.016   0.016   0.016   0.016   0.016   0.016   0.016   0.016   0.123    0.016    0.016    0.016   \n",
      "sms4    0.014   0.014   0.014   0.014   0.014   0.014   0.178   0.014   0.014   0.014    0.014    0.014    0.014   \n",
      "sms5!   0.009   0.009   0.009   0.009   0.009   0.009   0.009   0.009   0.009   0.009    0.009    0.009    0.009   \n",
      "\n",
      "       topic13  topic14  topic15  \n",
      "sms0     0.215    0.011    0.011  \n",
      "sms1     0.018    0.018    0.018  \n",
      "sms2!    0.010    0.010    0.010  \n",
      "sms3     0.103    0.016    0.016  \n",
      "sms4     0.621    0.014    0.014  \n",
      "sms5!    0.858    0.009    0.009  \n",
      "step 4 =  Accuracy: 0.875 (+/-0.033)\n"
     ]
    }
   ],
   "source": [
    "# LDiA\n",
    "# do the same as above but with LDiA, however use Latent Dirichlet Allocation (LDA) instead of PCA\n",
    "\n",
    "# make tf-idf vectors\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.tokenize.casual import casual_tokenize\n",
    "tfidf = TfidfVectorizer(tokenizer=casual_tokenize)\n",
    "tfidf_docs = tfidf.fit_transform(raw_documents=sms.text).toarray()\n",
    "print(\"step 2 = \", tfidf_docs)\n",
    "\n",
    "# do lsa using pca\n",
    "from sklearn.decomposition import LatentDirichletAllocation as LDiA\n",
    "ldia = LDiA(n_components=16)\n",
    "ldia = ldia.fit(tfidf_docs)\n",
    "ldia_topic_vectors = ldia.transform(tfidf_docs)\n",
    "columns = ['topic{}'.format(i) for i in range(ldia.n_components)]\n",
    "ldia_topic_vectors = pd.DataFrame(ldia_topic_vectors, index=index, columns=columns)\n",
    "print(\"step 3 = \", ldia_topic_vectors.round(3).head(6))\n",
    "\n",
    "# do lda with validation\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "lda = LDA(n_components=1)\n",
    "scores = cross_val_score(lda, ldia_topic_vectors, sms.spam, cv=10)\n",
    "print(\"step 4 = \", \"Accuracy: {:.3f} (+/-{:.3f})\".format(scores.mean(), scores.std()*2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 2 =  [[0.03005694 0.03005694 0.45085413 0.2103986  0.04224398 0.04224398\n",
      "  0.04224398 0.04224398 0.04224398 0.         0.04224398 0.\n",
      "  0.         0.         0.         0.04224398 0.         0.\n",
      "  0.04224398 0.03005694 0.08448797 0.04224398 0.         0.03005694\n",
      "  0.15028471 0.         0.03005694 0.03005694 0.03005694 0.\n",
      "  0.04224398 0.04224398 0.04224398 0.08448797 0.04224398 0.04224398\n",
      "  0.         0.04224398 0.04224398 0.04224398 0.08448797 0.09017083\n",
      "  0.         0.         0.04224398 0.         0.04224398 0.\n",
      "  0.04224398 0.         0.         0.04224398 0.         0.\n",
      "  0.         0.         0.         0.         0.         0.08448797\n",
      "  0.04224398 0.         0.04224398 0.         0.04224398 0.04224398\n",
      "  0.04224398 0.04224398 0.04224398 0.         0.04224398 0.\n",
      "  0.08448797 0.06011388 0.         0.04224398 0.04224398 0.04224398\n",
      "  0.04224398 0.         0.         0.04224398 0.04224398 0.\n",
      "  0.04224398 0.03005694 0.24045554 0.         0.04224398 0.\n",
      "  0.06011388 0.03005694 0.06011388 0.04224398 0.04224398 0.\n",
      "  0.04224398 0.         0.04224398 0.         0.         0.\n",
      "  0.         0.         0.04224398 0.         0.         0.\n",
      "  0.04224398 0.         0.         0.         0.08448797 0.04224398\n",
      "  0.04224398 0.27051248 0.         0.04224398 0.08448797 0.03005694\n",
      "  0.         0.         0.04224398 0.03005694 0.08448797 0.\n",
      "  0.04224398 0.         0.         0.         0.         0.04224398\n",
      "  0.08448797 0.04224398 0.         0.04224398 0.04224398 0.04224398\n",
      "  0.04224398 0.08448797 0.04224398 0.04224398 0.         0.\n",
      "  0.04224398 0.04224398 0.         0.         0.         0.\n",
      "  0.         0.         0.         0.04224398 0.04224398 0.\n",
      "  0.51096802 0.04224398 0.04224398 0.         0.06011388 0.04224398\n",
      "  0.         0.04224398 0.16897593 0.04224398 0.         0.\n",
      "  0.04224398 0.2534639  0.04224398 0.04224398 0.04224398 0.03005694\n",
      "  0.         0.         0.08448797 0.09017083 0.04224398]\n",
      " [0.02350803 0.02350803 0.56419275 0.14104819 0.         0.\n",
      "  0.         0.         0.         0.03303972 0.         0.03303972\n",
      "  0.03303972 0.03303972 0.03303972 0.         0.03303972 0.03303972\n",
      "  0.         0.02350803 0.         0.         0.03303972 0.02350803\n",
      "  0.11754016 0.03303972 0.11754016 0.02350803 0.02350803 0.03303972\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.06607944 0.         0.         0.         0.         0.02350803\n",
      "  0.06607944 0.06607944 0.         0.06607944 0.         0.23127803\n",
      "  0.         0.03303972 0.03303972 0.         0.03303972 0.03303972\n",
      "  0.03303972 0.03303972 0.03303972 0.03303972 0.03303972 0.\n",
      "  0.         0.03303972 0.         0.03303972 0.         0.\n",
      "  0.         0.         0.         0.03303972 0.         0.03303972\n",
      "  0.         0.04701606 0.03303972 0.         0.         0.\n",
      "  0.         0.06607944 0.03303972 0.         0.         0.03303972\n",
      "  0.         0.02350803 0.14104819 0.03303972 0.         0.03303972\n",
      "  0.14104819 0.04701606 0.04701606 0.         0.         0.03303972\n",
      "  0.         0.03303972 0.         0.03303972 0.03303972 0.03303972\n",
      "  0.03303972 0.09911915 0.         0.03303972 0.06607944 0.13215887\n",
      "  0.         0.03303972 0.26431774 0.03303972 0.         0.\n",
      "  0.         0.18806425 0.03303972 0.         0.         0.02350803\n",
      "  0.03303972 0.06607944 0.         0.02350803 0.         0.03303972\n",
      "  0.         0.03303972 0.03303972 0.03303972 0.06607944 0.\n",
      "  0.         0.         0.03303972 0.         0.         0.\n",
      "  0.         0.         0.         0.         0.03303972 0.03303972\n",
      "  0.         0.         0.03303972 0.03303972 0.03303972 0.03303972\n",
      "  0.06607944 0.06607944 0.06607944 0.         0.         0.03303972\n",
      "  0.47016063 0.         0.         0.03303972 0.04701606 0.\n",
      "  0.03303972 0.         0.         0.         0.09911915 0.03303972\n",
      "  0.         0.         0.         0.         0.         0.04701606\n",
      "  0.13215887 0.03303972 0.         0.18806425 0.        ]]\n",
      "step 3 =           topic0\n",
      "york      0.403\n",
      "newyork  -0.403\n",
      "step 4 =  [[0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.81480247\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.57973867 0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.         0.\n",
      "  0.         0.         0.         0.         0.        ]]\n",
      "step 5 =  [[-0.29215675]]\n",
      "step 6 =  0.703719465708049\n"
     ]
    }
   ],
   "source": [
    "# write toy model of semantic search using following setup:\n",
    "# 1. read corpus consistng of Wikipedia info as 2 separate files\n",
    "# YorkAustralia.txt and NewYork.txt\n",
    "# 2. make tf-idf vectors for corpus\n",
    "# 3. do 1 topic analysis - it will be New Yourk'iness or York'iness\n",
    "# 4. make question 'What is biggest city?'\n",
    "# make tf-idf vector for question\n",
    "# do pca transform\n",
    "# 5. check which topic/document is closer to question\n",
    "\n",
    "import pandas as pd\n",
    "pd.options.display.width = 120\n",
    "import numpy as np\n",
    "from nltk.tokenize.casual import casual_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# read corpus\n",
    "with open('YorkAustralia.txt', 'r') as f:\n",
    "    york = f.read()\n",
    "with open('NewYork.txt', 'r') as f:\n",
    "    newyork = f.read()\n",
    "corpus = [york, newyork]\n",
    "index = ['york', 'newyork']\n",
    "\n",
    "# make tf-idf vectors\n",
    "tfidf = TfidfVectorizer(tokenizer=casual_tokenize)\n",
    "tfidf_docs = tfidf.fit_transform(raw_documents=corpus).toarray()\n",
    "print(\"step 2 = \", tfidf_docs)\n",
    "\n",
    "# do lsa using pca\n",
    "pca = PCA(n_components=1)\n",
    "pca = pca.fit(tfidf_docs)\n",
    "pca_topic_vectors = pca.transform(tfidf_docs)\n",
    "columns = ['topic{}'.format(i) for i in range(pca.n_components)]\n",
    "pca_topic_vectors = pd.DataFrame(pca_topic_vectors, index=index, columns=columns)\n",
    "print(\"step 3 = \", pca_topic_vectors.round(3).head(6))\n",
    "\n",
    "# make question\n",
    "question = 'What is biggest city?'\n",
    "\n",
    "# make tf-idf vector for question\n",
    "tfidf_question = tfidf.transform([question]).toarray()\n",
    "print(\"step 4 = \", tfidf_question)\n",
    "\n",
    "# do pca transform\n",
    "pca_question = pca.transform(tfidf_question)\n",
    "print(\"step 5 = \", pca_question)\n",
    "\n",
    "# check which topic/document is closer to question\n",
    "print(\"step 6 = \", np.linalg.norm(pca_question - pca_topic_vectors))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.2 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0abe801034007484f152c408f6878125c4d199d6c578a45ceffdae6ced931ee7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
